{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.dataset import *\n",
    "from models.meta import Meta\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import timm\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from models.basenet import *\n",
    "from utils import *\n",
    "from configs.config_setting_baseline import setting_config\n",
    "from copy import deepcopy\n",
    "import sklearn.metrics as metrics\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.nn.init as init\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "config = setting_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(batch):\n",
    "    support_images = batch['support_images'].squeeze(0)\n",
    "    support_masks = batch['support_masks'].squeeze(0)\n",
    "    query_images = batch['query_images'].squeeze(0)\n",
    "    query_masks = batch['query_masks'].squeeze(0)\n",
    "    return support_images, support_masks, query_images, query_masks\n",
    "\n",
    "# the function of copying the images\n",
    "def copy_file_to_folder(source_file, dest_folder):\n",
    "    if not os.path.exists(dest_folder):\n",
    "        os.makedirs(dest_folder)\n",
    "\n",
    "    dest_path = os.path.join(dest_folder, os.path.basename(source_file))\n",
    "    shutil.copy(source_file, dest_path)\n",
    "\n",
    "def evaluation_api(predicted_list,groudtruth_list):\n",
    "    pre = np.array([item for sublist in predicted_list for item in sublist]).reshape(-1)\n",
    "    gts = np.array([item for sublist in groudtruth_list for item in sublist]).reshape(-1)\n",
    "    # confusion_matrix = metrics.confusion_matrix(gts,pre)\n",
    "    # TN, FP, FN, TP = confusion[0,0], confusion[0,1], confusion[1,0], confusion[1,1] \n",
    "    dice = metrics.f1_score(gts,pre)\n",
    "\n",
    "    return dice\n",
    "\n",
    "def evaluation_epoch(predicted_list,groundtruth_list):\n",
    "    TP = [0]*config.num_classes\n",
    "    FP = [0]*config.num_classes\n",
    "    FN = [0]*config.num_classes\n",
    "    dice = [0.0]*config.num_classes\n",
    "    \n",
    "    for i in range(len(predicted_list)):\n",
    "        preds = np.array(predicted_list[i]).reshape(-1)\n",
    "        gts = np.array(groundtruth_list[i]).reshape(-1)\n",
    "        for j in range(len(preds)):\n",
    "            if preds[j] == gts[j]:\n",
    "                TP[gts[j]] += 1\n",
    "            else:\n",
    "                FP[preds[j]] += 1\n",
    "                FN[gts[j]] += 1        \n",
    "    \n",
    "    for i in range(config.num_classes):\n",
    "        dice[i] = (2 * TP[i])/(FP[i]+FN[i]+2*TP[i]+1)\n",
    "\n",
    "    mdice = (2*np.sum(TP))/(np.sum(FP)+np.sum(FN)+2*np.sum(TP)+1)    \n",
    "    return dice,mdice\n",
    "\n",
    "def evaluation_basenet(base_net,query_images,query_masks,criterion):\n",
    "    predicted = base_net(query_images)\n",
    "    loss = criterion(predicted,query_masks)\n",
    "    predicted = torch.argmax(predicted,dim=1).long()\n",
    "    predict_numpy = predicted.detach().cpu().numpy().reshape(-1)\n",
    "    masks_numpy = query_masks.long().detach().cpu().numpy().reshape(-1)\n",
    "    accuracy = metrics.accuracy_score(masks_numpy,predict_numpy)\n",
    "    f1_score = metrics.f1_score(masks_numpy,predict_numpy,average=None)\n",
    "    return accuracy,f1_score,loss\n",
    "\n",
    "def initialize_weights_he(model):\n",
    "    for param in model.parameters():\n",
    "        init.kaiming_uniform_(param, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "def initialize_weights_xavier(model):\n",
    "    for param in model.parameters():\n",
    "        init.xavier_uniform_(param)\n",
    "\n",
    "def initialize_weights_normal(model):\n",
    "    for param in model.parameters():\n",
    "        init.normal_(param, mean=0, std=1)\n",
    "\n",
    "def remove_exsits_folder(folderpath):\n",
    "    if os.path.exists(folderpath):\n",
    "        shutil.rmtree(folderpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------Creating logger----------#\n"
     ]
    }
   ],
   "source": [
    "print('#----------Creating logger----------#')\n",
    "sys.path.append(config.work_dir + '/')\n",
    "log_dir = os.path.join(config.work_dir, 'log')\n",
    "checkpoint_dir = os.path.join(config.work_dir, 'checkpoints')\n",
    "resume_model = os.path.join(checkpoint_dir, 'latest.pth')\n",
    "outputs = os.path.join(config.work_dir, 'outputs')\n",
    "csv_save = os.path.join(config.work_dir, 'csv')\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "if not os.path.exists(outputs):\n",
    "    os.makedirs(outputs)\n",
    "if not os.path.exists(csv_save):\n",
    "    os.makedirs(csv_save)\n",
    "\n",
    "global logger\n",
    "logger = get_logger('test', log_dir)\n",
    "global writer\n",
    "writer = SummaryWriter(config.work_dir + 'summary')\n",
    "\n",
    "log_config_info(config, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------Generating data----------#\n"
     ]
    }
   ],
   "source": [
    "print('#----------Generating data----------#')\n",
    "images_resources_path = \"./data/HAM10000/origin/images/\"         # the resource folder of images\n",
    "masks_resources_path = \"./data/HAM10000/origin/masks/\"           # the resource folder of masks\n",
    "ratio = [0.6,0.2]     # the ratio point of train dataset and validation set and testset\n",
    "categories = config.categories\n",
    "categories_dictionary = {}\n",
    "category_id = 1\n",
    "# prepare the csv for groundtruth\n",
    "origin_groundtruth_csv = \"./data/HAM10000/origin/groundtruth/HAM10000_groundtruth.csv\"   # read the csv file\n",
    "origin_groundtruth = pd.read_csv(origin_groundtruth_csv)    # read the csv file of groundtruth\n",
    "\n",
    "# generating the folders for each category in train folder and test folder\n",
    "# create folders for each categories\n",
    "trainset_images_path = \"./data/HAM10000/train/images/\"     # the images path for train dataset\n",
    "trainset_masks_path = \"./data/HAM10000/train/masks/\"     # the masks path for train dataset\n",
    "valset_images_path = \"./data/HAM10000/val/images/\"     # the images path for validation dataset\n",
    "valset_masks_path = \"./data/HAM10000/val/masks/\"      # the masks path for validation dataset\n",
    "testset_images_path = \"./data/HAM10000/test/images/\"     # the images path for test dataset\n",
    "testset_masks_path = \"./data/HAM10000/test/masks/\"      # the masks path for test dataset\n",
    "\n",
    "for category in categories:\n",
    "    # prepare the address for folders\n",
    "    category_images_train_path = os.path.join(trainset_images_path,category)\n",
    "    category_masks_train_path = os.path.join(trainset_masks_path,category)\n",
    "    category_images_val_path = os.path.join(valset_images_path,category)\n",
    "    category_masks_val_path = os.path.join(valset_masks_path,category)\n",
    "    category_images_test_path = os.path.join(testset_images_path,category)\n",
    "    category_masks_test_path = os.path.join(testset_masks_path,category)\n",
    "    #delete the previously exsited folders\n",
    "    remove_exsits_folder(category_images_train_path)\n",
    "    remove_exsits_folder(category_masks_train_path)\n",
    "    remove_exsits_folder(category_images_val_path)\n",
    "    remove_exsits_folder(category_masks_val_path)\n",
    "    remove_exsits_folder(category_images_test_path)\n",
    "    remove_exsits_folder(category_masks_test_path)\n",
    "    # create corresponding folder for each categories\n",
    "    os.makedirs(category_images_train_path, exist_ok=True)\n",
    "    os.makedirs(category_masks_train_path, exist_ok=True)\n",
    "    os.makedirs(category_images_val_path, exist_ok=True)\n",
    "    os.makedirs(category_masks_val_path, exist_ok=True)\n",
    "    os.makedirs(category_images_test_path, exist_ok=True)\n",
    "    os.makedirs(category_masks_test_path, exist_ok=True)\n",
    "\n",
    "    # generate the data in trainset and testset for each categories\n",
    "    dest_folder_images = \"./data/HAM10000/train/images/\"+category    # the destination train set folder of copying the images\n",
    "    dest_folder_masks = \"./data/HAM10000/train/masks/\"+category    # the destination trian set folder of copying the masks\n",
    "    dest_folder_images_change_val = \"./data/HAM10000/val/images/\"+category     # the destination folder of test set images\n",
    "    dest_folder_masks_change_val = \"./data/HAM10000/val/masks/\"+category      # the destination folder of test set masks\n",
    "    dest_folder_images_change_test = \"./data/HAM10000/test/images/\"+category     # the destination folder of test set images\n",
    "    dest_folder_masks_change_test = \"./data/HAM10000/test/masks/\"+category      # the destination folder of test set masks\n",
    "    data_categories = origin_groundtruth[origin_groundtruth['dx'] == category]      # extract each categories \n",
    "    data_categories = data_categories.sample(frac=1,random_state=config.seed)       # random sample the datagenerating\n",
    "    length_categories = len(data_categories)\n",
    "    change_folder_point_valset = math.floor(length_categories * ratio[0])     # get the point to change directory name\n",
    "    change_folder_point_testset = math.floor(length_categories * (ratio[0]+ratio[1]))     # get the point to change directory name \n",
    "    elements_count = 0\n",
    "    for image_name in data_categories['image_id']:      # each image_id in each categories\n",
    "        if elements_count == change_folder_point_valset:\n",
    "            dest_folder_images = dest_folder_images_change_val\n",
    "            dest_folder_masks = dest_folder_masks_change_val\n",
    "        elif elements_count == change_folder_point_testset:\n",
    "            dest_folder_images = dest_folder_images_change_test\n",
    "            dest_folder_masks = dest_folder_masks_change_test\n",
    "        images_file = image_name+\".jpg\"\n",
    "        masks_file = image_name+\"_segmentation.png\"\n",
    "        source_image = images_resources_path+images_file    # the full path of source of image : path + image file name\n",
    "        source_mask = masks_resources_path+masks_file       # the full path of source of mask : path + mask file name\n",
    "        copy_file_to_folder(source_image,dest_folder_images)\n",
    "        # masks should be preprocess to the form of output for network (Width*Height*Category)\n",
    "        image = Image.open(source_mask)\n",
    "        image_array = np.array(image)\n",
    "        image_array[image_array == 255] = 1\n",
    "        image = Image.fromarray(image_array)\n",
    "        image.save(os.path.join(dest_folder_masks, masks_file))\n",
    "        elements_count +=1\n",
    "    categories_dictionary[category] = category_id       # add the category id in the categories_dictionary\n",
    "    category_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------GPU init----------#\n"
     ]
    }
   ],
   "source": [
    "print('#----------GPU init----------#')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu_id\n",
    "set_seed(config.seed)\n",
    "device = torch.device('cuda')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------Prepareing Datasets----------#\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39m# create the dataset and dataloader\u001b[39;00m\n\u001b[1;32m      3\u001b[0m batch_size \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mbatch_size\n\u001b[0;32m----> 4\u001b[0m train_dataset \u001b[39m=\u001b[39m HAMALL_datasets(config, train\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      5\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(\n\u001b[1;32m      6\u001b[0m     train_dataset, batch_size\u001b[39m=\u001b[39mbatch_size, num_workers\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mnum_workers)\n\u001b[1;32m      7\u001b[0m val_dataset \u001b[39m=\u001b[39m HAMALL_datasets(config, train\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/mnt/sdb/sda/wj/Segment-All-on-Skin/datasets/dataset.py:224\u001b[0m, in \u001b[0;36mHAMALL_datasets.__init__\u001b[0;34m(self, config, train, categories, num_eachcat)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_to_samples \u001b[39m=\u001b[39m {\u001b[39mcls\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_samples(\u001b[39mcls\u001b[39m) \u001b[39mfor\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcategories}\n\u001b[1;32m    223\u001b[0m \u001b[39m# get the final datalist \u001b[39;00m\n\u001b[0;32m--> 224\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_datalist()\n",
      "File \u001b[0;32m/mnt/sdb/sda/wj/Segment-All-on-Skin/datasets/dataset.py:267\u001b[0m, in \u001b[0;36mHAMALL_datasets.create_datalist\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m temp_datalist \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_task()\n\u001b[1;32m    266\u001b[0m \u001b[39mfor\u001b[39;00m i,sample \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(temp_datalist):\n\u001b[0;32m--> 267\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(Image\u001b[39m.\u001b[39;49mopen(sample[\u001b[39m'\u001b[39;49m\u001b[39mimage\u001b[39;49m\u001b[39m'\u001b[39;49m])\u001b[39m.\u001b[39;49mconvert(\u001b[39m'\u001b[39;49m\u001b[39mRGB\u001b[39;49m\u001b[39m'\u001b[39;49m))\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m    268\u001b[0m     mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask_transform(Image\u001b[39m.\u001b[39mopen(sample[\u001b[39m'\u001b[39m\u001b[39mmask\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mconvert(\u001b[39m'\u001b[39m\u001b[39mL\u001b[39m\u001b[39m'\u001b[39m))\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m    269\u001b[0m     temp_datapair \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m: image, \u001b[39m'\u001b[39m\u001b[39mmask\u001b[39m\u001b[39m'\u001b[39m: mask}\n",
      "File \u001b[0;32m~/anaconda3/envs/dataEngineering/lib/python3.11/site-packages/PIL/Image.py:937\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert\u001b[39m(\n\u001b[1;32m    890\u001b[0m     \u001b[39mself\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, matrix\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dither\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, palette\u001b[39m=\u001b[39mPalette\u001b[39m.\u001b[39mWEB, colors\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m\n\u001b[1;32m    891\u001b[0m ):\n\u001b[1;32m    892\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    893\u001b[0m \u001b[39m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[1;32m    894\u001b[0m \u001b[39m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[39m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m    935\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 937\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m    939\u001b[0m     has_transparency \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtransparency\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    940\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m mode \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mP\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    941\u001b[0m         \u001b[39m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dataEngineering/lib/python3.11/site-packages/PIL/ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(msg)\n\u001b[1;32m    268\u001b[0m b \u001b[39m=\u001b[39m b \u001b[39m+\u001b[39m s\n\u001b[0;32m--> 269\u001b[0m n, err_code \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39;49mdecode(b)\n\u001b[1;32m    270\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    271\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('#----------Prepareing Datasets----------#')\n",
    "# create the dataset and dataloader\n",
    "batch_size = config.batch_size\n",
    "train_dataset = HAMALL_datasets(config, train=True)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, num_workers=config.num_workers)\n",
    "val_dataset = HAMALL_datasets(config, train=False)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size, num_workers=config.num_workers)\n",
    "test_dataset = HAMALL_datasets(config, train=False)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, num_workers=config.num_workers)\n",
    "print(\"trian_dataset length:\",len(train_dataset))\n",
    "print(\"val_dataset length:\",len(val_dataset))\n",
    "print(\"test_dataset length:\",len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------Prepareing Model----------#\n"
     ]
    }
   ],
   "source": [
    "print('#----------Prepareing Model----------#')\n",
    "in_channels = config.in_channels\n",
    "out_channels = config.out_channels\n",
    "base_net = smp.Unet(encoder_name='resnet34', encoder_depth=5, encoder_weights=None, decoder_use_batchnorm=True, decoder_channels=(256, 128, 64, 32, 16), decoder_attention_type=None, in_channels=3, classes=config.out_channels, activation=None, aux_params=None)\n",
    "# base_net = smp.UnetPlusPlus(encoder_name='resnet34', encoder_depth=5, encoder_weights=None, decoder_use_batchnorm=True, decoder_channels=(256, 128, 64, 32, 16), decoder_attention_type=None, in_channels=3, classes=config.out_channels, activation=None, aux_params=None)\n",
    "# initialize_weights_he(base_net)\n",
    "base_net = base_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------Prepareing loss, opt, sch and amp----------#\n"
     ]
    }
   ],
   "source": [
    "print('#----------Prepareing loss, opt, sch and amp----------#')\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "meta_optimizer = get_optimizer(config, base_net)\n",
    "meta_scheduler = get_scheduler(config, meta_optimizer)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------Set other params----------#\n"
     ]
    }
   ],
   "source": [
    "print('#----------Set other params----------#')\n",
    "min_loss = 999\n",
    "start_epoch = 1\n",
    "min_epoch = 1\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------Start training----------#\n",
      "128-resizeh, 128-resizew, 0.000100-outer_lr\n",
      "#Train#  epoch: 1, loss: 0.5402318239212036, dice: 0\n",
      "#Val#  epoch: 1, dice: 0\n",
      "#Test#  epoch: 1, dice: 0\n",
      "#Train#  epoch: 2, loss: 0.4137924909591675, dice: 0\n",
      "#Val#  epoch: 2, dice: 0\n",
      "#Test#  epoch: 2, dice: 0\n",
      "#Train#  epoch: 3, loss: 0.3548763692378998, dice: 0\n",
      "#Val#  epoch: 3, dice: 0\n",
      "#Test#  epoch: 3, dice: 0\n",
      "#Train#  epoch: 4, loss: 0.32274630665779114, dice: 0\n",
      "#Val#  epoch: 4, dice: 0\n",
      "#Test#  epoch: 4, dice: 0\n",
      "#Train#  epoch: 5, loss: 0.30549389123916626, dice: 0\n",
      "#Val#  epoch: 5, dice: 0\n",
      "#Test#  epoch: 5, dice: 0\n",
      "#Train#  epoch: 6, loss: 0.2790088951587677, dice: 0\n",
      "#Val#  epoch: 6, dice: 0\n",
      "#Test#  epoch: 6, dice: 0\n",
      "#Train#  epoch: 7, loss: 0.2535657286643982, dice: 0\n",
      "#Val#  epoch: 7, dice: 0\n",
      "#Test#  epoch: 7, dice: 0\n",
      "#Train#  epoch: 8, loss: 0.23556016385555267, dice: 0\n",
      "#Val#  epoch: 8, dice: 0\n",
      "#Test#  epoch: 8, dice: 0\n",
      "#Train#  epoch: 9, loss: 0.21944865584373474, dice: 0\n",
      "#Val#  epoch: 9, dice: 0\n",
      "#Test#  epoch: 9, dice: 0\n",
      "#Train#  epoch: 10, loss: 0.20020824670791626, dice: 0\n",
      "#Val#  epoch: 10, dice: 0\n",
      "#Test#  epoch: 10, dice: 0\n",
      "#Train#  epoch: 11, loss: 0.19663554430007935, dice: 0\n",
      "#Val#  epoch: 11, dice: 0\n",
      "#Test#  epoch: 11, dice: 0\n",
      "#Train#  epoch: 12, loss: 0.1876799762248993, dice: 0\n",
      "#Val#  epoch: 12, dice: 0\n",
      "#Test#  epoch: 12, dice: 0\n",
      "#Train#  epoch: 13, loss: 0.17346253991127014, dice: 0\n",
      "#Val#  epoch: 13, dice: 0\n",
      "#Test#  epoch: 13, dice: 0\n",
      "#Train#  epoch: 14, loss: 0.16073991358280182, dice: 0\n",
      "#Val#  epoch: 14, dice: 0\n",
      "#Test#  epoch: 14, dice: 0\n",
      "#Train#  epoch: 15, loss: 0.1474146544933319, dice: 0\n",
      "#Val#  epoch: 15, dice: 0\n",
      "#Test#  epoch: 15, dice: 0\n",
      "#Train#  epoch: 16, loss: 0.1341005563735962, dice: 0\n",
      "#Val#  epoch: 16, dice: 0\n",
      "#Test#  epoch: 16, dice: 0\n",
      "#Train#  epoch: 17, loss: 0.12260466068983078, dice: 0\n",
      "#Val#  epoch: 17, dice: 0\n",
      "#Test#  epoch: 17, dice: 0\n",
      "#Train#  epoch: 18, loss: 0.11648485064506531, dice: 0\n",
      "#Val#  epoch: 18, dice: 0\n",
      "#Test#  epoch: 18, dice: 0\n",
      "#Train#  epoch: 19, loss: 0.1195385530591011, dice: 0\n",
      "#Val#  epoch: 19, dice: 0\n",
      "#Test#  epoch: 19, dice: 0\n",
      "#Train#  epoch: 20, loss: 0.11464749276638031, dice: 0\n",
      "#Val#  epoch: 20, dice: 0\n",
      "#Test#  epoch: 20, dice: 0\n",
      "#Train#  epoch: 21, loss: 0.10867403447628021, dice: 0\n",
      "#Val#  epoch: 21, dice: 0\n",
      "#Test#  epoch: 21, dice: 0\n",
      "#Train#  epoch: 22, loss: 0.09704908728599548, dice: 0\n",
      "#Val#  epoch: 22, dice: 0\n",
      "#Test#  epoch: 22, dice: 0\n",
      "#Train#  epoch: 23, loss: 0.09039925038814545, dice: 0\n",
      "#Val#  epoch: 23, dice: 0\n",
      "#Test#  epoch: 23, dice: 0\n",
      "#Train#  epoch: 24, loss: 0.08654402196407318, dice: 0\n",
      "#Val#  epoch: 24, dice: 0\n",
      "#Test#  epoch: 24, dice: 0\n",
      "#Train#  epoch: 25, loss: 0.08487153053283691, dice: 0\n",
      "#Val#  epoch: 25, dice: 0\n",
      "#Test#  epoch: 25, dice: 0\n",
      "#Train#  epoch: 26, loss: 0.08756710588932037, dice: 0\n",
      "#Val#  epoch: 26, dice: 0\n",
      "#Test#  epoch: 26, dice: 0\n",
      "#Train#  epoch: 27, loss: 0.08314915746450424, dice: 0\n",
      "#Val#  epoch: 27, dice: 0\n",
      "#Test#  epoch: 27, dice: 0\n",
      "#Train#  epoch: 28, loss: 0.0806807428598404, dice: 0\n",
      "#Val#  epoch: 28, dice: 0\n",
      "#Test#  epoch: 28, dice: 0\n",
      "#Train#  epoch: 29, loss: 0.08035612851381302, dice: 0\n",
      "#Val#  epoch: 29, dice: 0\n",
      "#Test#  epoch: 29, dice: 0\n",
      "#Train#  epoch: 30, loss: 0.07232349365949631, dice: 0\n",
      "#Val#  epoch: 30, dice: 0\n",
      "#Test#  epoch: 30, dice: 0\n",
      "#Train#  epoch: 31, loss: 0.0700400322675705, dice: 0\n",
      "#Val#  epoch: 31, dice: 0\n",
      "#Test#  epoch: 31, dice: 0\n",
      "#Train#  epoch: 32, loss: 0.06831412762403488, dice: 0\n",
      "#Val#  epoch: 32, dice: 0\n",
      "#Test#  epoch: 32, dice: 0\n",
      "#Train#  epoch: 33, loss: 0.06513307988643646, dice: 0\n",
      "#Val#  epoch: 33, dice: 0\n",
      "#Test#  epoch: 33, dice: 0\n",
      "#Train#  epoch: 34, loss: 0.06368999928236008, dice: 0\n",
      "#Val#  epoch: 34, dice: 0\n",
      "#Test#  epoch: 34, dice: 0\n",
      "#Train#  epoch: 35, loss: 0.06606467068195343, dice: 0\n",
      "#Val#  epoch: 35, dice: 0\n",
      "#Test#  epoch: 35, dice: 0\n",
      "#Train#  epoch: 36, loss: 0.06598425656557083, dice: 0\n",
      "#Val#  epoch: 36, dice: 0\n",
      "#Test#  epoch: 36, dice: 0\n",
      "#Train#  epoch: 37, loss: 0.06593355536460876, dice: 0\n",
      "#Val#  epoch: 37, dice: 0\n",
      "#Test#  epoch: 37, dice: 0\n",
      "#Train#  epoch: 38, loss: 0.06404706090688705, dice: 0\n",
      "#Val#  epoch: 38, dice: 0\n",
      "#Test#  epoch: 38, dice: 0\n",
      "#Train#  epoch: 39, loss: 0.06174542382359505, dice: 0\n",
      "#Val#  epoch: 39, dice: 0\n",
      "#Test#  epoch: 39, dice: 0\n",
      "#Train#  epoch: 40, loss: 0.05986969918012619, dice: 0\n",
      "#Val#  epoch: 40, dice: 0\n",
      "#Test#  epoch: 40, dice: 0\n",
      "#Train#  epoch: 41, loss: 0.05791216343641281, dice: 0\n",
      "#Val#  epoch: 41, dice: 0\n",
      "#Test#  epoch: 41, dice: 0\n",
      "#Train#  epoch: 42, loss: 0.05679618939757347, dice: 0\n",
      "#Val#  epoch: 42, dice: 0\n",
      "#Test#  epoch: 42, dice: 0\n",
      "#Train#  epoch: 43, loss: 0.055728912353515625, dice: 0\n",
      "#Val#  epoch: 43, dice: 0\n",
      "#Test#  epoch: 43, dice: 0\n",
      "#Train#  epoch: 44, loss: 0.06415599584579468, dice: 0\n",
      "#Val#  epoch: 44, dice: 0\n",
      "#Test#  epoch: 44, dice: 0\n",
      "#Train#  epoch: 45, loss: 0.1026371642947197, dice: 0\n",
      "#Val#  epoch: 45, dice: 0\n",
      "#Test#  epoch: 45, dice: 0\n",
      "#Train#  epoch: 46, loss: 0.13040055334568024, dice: 0\n",
      "#Val#  epoch: 46, dice: 0\n",
      "#Test#  epoch: 46, dice: 0\n",
      "#Train#  epoch: 47, loss: 0.12905865907669067, dice: 0\n",
      "#Val#  epoch: 47, dice: 0\n",
      "#Test#  epoch: 47, dice: 0\n",
      "#Train#  epoch: 48, loss: 0.1128474622964859, dice: 0\n",
      "#Val#  epoch: 48, dice: 0\n",
      "#Test#  epoch: 48, dice: 0\n",
      "#Train#  epoch: 49, loss: 0.10335422307252884, dice: 0\n",
      "#Val#  epoch: 49, dice: 0\n",
      "#Test#  epoch: 49, dice: 0\n",
      "#Train#  epoch: 50, loss: 0.08883336186408997, dice: 0\n",
      "#Val#  epoch: 50, dice: 0\n",
      "#Test#  epoch: 50, dice: 0\n",
      "#Train#  epoch: 51, loss: 0.07490769028663635, dice: 0\n",
      "#Val#  epoch: 51, dice: 0\n",
      "#Test#  epoch: 51, dice: 0\n",
      "#Train#  epoch: 52, loss: 0.06578531861305237, dice: 0\n",
      "#Val#  epoch: 52, dice: 0\n",
      "#Test#  epoch: 52, dice: 0\n",
      "#Train#  epoch: 53, loss: 0.058712247759103775, dice: 0\n",
      "#Val#  epoch: 53, dice: 0\n",
      "#Test#  epoch: 53, dice: 0\n",
      "#Train#  epoch: 54, loss: 0.05808962881565094, dice: 0\n",
      "#Val#  epoch: 54, dice: 0\n",
      "#Test#  epoch: 54, dice: 0\n",
      "#Train#  epoch: 55, loss: 0.05854712054133415, dice: 0\n",
      "#Val#  epoch: 55, dice: 0\n",
      "#Test#  epoch: 55, dice: 0\n",
      "#Train#  epoch: 56, loss: 0.05377918481826782, dice: 0\n",
      "#Val#  epoch: 56, dice: 0\n",
      "#Test#  epoch: 56, dice: 0\n",
      "#Train#  epoch: 57, loss: 0.05028575286269188, dice: 0\n",
      "#Val#  epoch: 57, dice: 0\n",
      "#Test#  epoch: 57, dice: 0\n",
      "#Train#  epoch: 58, loss: 0.046826377511024475, dice: 0\n",
      "#Val#  epoch: 58, dice: 0\n",
      "#Test#  epoch: 58, dice: 0\n",
      "#Train#  epoch: 59, loss: 0.04692387953400612, dice: 0\n",
      "#Val#  epoch: 59, dice: 0\n",
      "#Test#  epoch: 59, dice: 0\n",
      "#Train#  epoch: 60, loss: 0.04657379165291786, dice: 0\n",
      "#Val#  epoch: 60, dice: 0\n",
      "#Test#  epoch: 60, dice: 0\n",
      "#Train#  epoch: 61, loss: 0.04662379249930382, dice: 0\n",
      "#Val#  epoch: 61, dice: 0\n",
      "#Test#  epoch: 61, dice: 0\n",
      "#Train#  epoch: 62, loss: 0.0451367162168026, dice: 0\n",
      "#Val#  epoch: 62, dice: 0\n",
      "#Test#  epoch: 62, dice: 0\n",
      "#Train#  epoch: 63, loss: 0.045756541192531586, dice: 0\n",
      "#Val#  epoch: 63, dice: 0\n",
      "#Test#  epoch: 63, dice: 0\n",
      "#Train#  epoch: 64, loss: 0.04595889523625374, dice: 0\n",
      "#Val#  epoch: 64, dice: 0\n",
      "#Test#  epoch: 64, dice: 0\n",
      "#Train#  epoch: 65, loss: 0.04539235308766365, dice: 0\n",
      "#Val#  epoch: 65, dice: 0\n",
      "#Test#  epoch: 65, dice: 0\n",
      "#Train#  epoch: 66, loss: 0.04094042629003525, dice: 0\n",
      "#Val#  epoch: 66, dice: 0\n",
      "#Test#  epoch: 66, dice: 0\n",
      "#Train#  epoch: 67, loss: 0.04119626432657242, dice: 0\n",
      "#Val#  epoch: 67, dice: 0\n",
      "#Test#  epoch: 67, dice: 0\n",
      "#Train#  epoch: 68, loss: 0.0417083241045475, dice: 0\n",
      "#Val#  epoch: 68, dice: 0\n",
      "#Test#  epoch: 68, dice: 0\n",
      "#Train#  epoch: 69, loss: 0.04455440863966942, dice: 0\n",
      "#Val#  epoch: 69, dice: 0\n",
      "#Test#  epoch: 69, dice: 0\n",
      "#Train#  epoch: 70, loss: 0.045208171010017395, dice: 0\n",
      "#Val#  epoch: 70, dice: 0\n",
      "#Test#  epoch: 70, dice: 0\n",
      "#Train#  epoch: 71, loss: 0.04143238812685013, dice: 0\n",
      "#Val#  epoch: 71, dice: 0\n",
      "#Test#  epoch: 71, dice: 0\n",
      "#Train#  epoch: 72, loss: 0.039099693298339844, dice: 0\n",
      "#Val#  epoch: 72, dice: 0\n",
      "#Test#  epoch: 72, dice: 0\n",
      "#Train#  epoch: 73, loss: 0.03741181641817093, dice: 0\n",
      "#Val#  epoch: 73, dice: 0\n",
      "#Test#  epoch: 73, dice: 0\n",
      "#Train#  epoch: 74, loss: 0.035499121993780136, dice: 0\n",
      "#Val#  epoch: 74, dice: 0\n",
      "#Test#  epoch: 74, dice: 0\n",
      "#Train#  epoch: 75, loss: 0.032788172364234924, dice: 0\n",
      "#Val#  epoch: 75, dice: 0\n",
      "#Test#  epoch: 75, dice: 0\n",
      "#Train#  epoch: 76, loss: 0.03326311707496643, dice: 0\n",
      "#Val#  epoch: 76, dice: 0\n",
      "#Test#  epoch: 76, dice: 0\n",
      "#Train#  epoch: 77, loss: 0.03264898434281349, dice: 0\n",
      "#Val#  epoch: 77, dice: 0\n",
      "#Test#  epoch: 77, dice: 0\n",
      "#Train#  epoch: 78, loss: 0.032366860657930374, dice: 0\n",
      "#Val#  epoch: 78, dice: 0\n",
      "#Test#  epoch: 78, dice: 0\n",
      "#Train#  epoch: 79, loss: 0.03193396329879761, dice: 0\n",
      "#Val#  epoch: 79, dice: 0\n",
      "#Test#  epoch: 79, dice: 0\n",
      "#Train#  epoch: 80, loss: 0.03064575605094433, dice: 0\n",
      "#Val#  epoch: 80, dice: 0\n",
      "#Test#  epoch: 80, dice: 0\n",
      "#Train#  epoch: 81, loss: 0.03139275312423706, dice: 0\n",
      "#Val#  epoch: 81, dice: 0\n",
      "#Test#  epoch: 81, dice: 0\n",
      "#Train#  epoch: 82, loss: 0.03198173642158508, dice: 0\n",
      "#Val#  epoch: 82, dice: 0\n",
      "#Test#  epoch: 82, dice: 0\n",
      "#Train#  epoch: 83, loss: 0.030158432200551033, dice: 0\n",
      "#Val#  epoch: 83, dice: 0\n",
      "#Test#  epoch: 83, dice: 0\n",
      "#Train#  epoch: 84, loss: 0.0315065011382103, dice: 0\n",
      "#Val#  epoch: 84, dice: 0\n",
      "#Test#  epoch: 84, dice: 0\n",
      "#Train#  epoch: 85, loss: 0.03205012530088425, dice: 0\n",
      "#Val#  epoch: 85, dice: 0\n",
      "#Test#  epoch: 85, dice: 0\n",
      "#Train#  epoch: 86, loss: 0.03387279063463211, dice: 0\n",
      "#Val#  epoch: 86, dice: 0\n",
      "#Test#  epoch: 86, dice: 0\n",
      "#Train#  epoch: 87, loss: 0.035225190222263336, dice: 0\n",
      "#Val#  epoch: 87, dice: 0\n",
      "#Test#  epoch: 87, dice: 0\n",
      "#Train#  epoch: 88, loss: 0.03662291169166565, dice: 0\n",
      "#Val#  epoch: 88, dice: 0\n",
      "#Test#  epoch: 88, dice: 0\n",
      "#Train#  epoch: 89, loss: 0.03854203224182129, dice: 0\n",
      "#Val#  epoch: 89, dice: 0\n",
      "#Test#  epoch: 89, dice: 0\n",
      "#Train#  epoch: 90, loss: 0.04037533700466156, dice: 0\n",
      "#Val#  epoch: 90, dice: 0\n",
      "#Test#  epoch: 90, dice: 0\n",
      "#Train#  epoch: 91, loss: 0.04463265463709831, dice: 0\n",
      "#Val#  epoch: 91, dice: 0\n",
      "#Test#  epoch: 91, dice: 0\n",
      "#Train#  epoch: 92, loss: 0.04114788398146629, dice: 0\n",
      "#Val#  epoch: 92, dice: 0\n",
      "#Test#  epoch: 92, dice: 0\n",
      "#Train#  epoch: 93, loss: 0.0366971492767334, dice: 0\n",
      "#Val#  epoch: 93, dice: 0\n",
      "#Test#  epoch: 93, dice: 0\n",
      "#Train#  epoch: 94, loss: 0.03402166813611984, dice: 0\n",
      "#Val#  epoch: 94, dice: 0\n",
      "#Test#  epoch: 94, dice: 0\n",
      "#Train#  epoch: 95, loss: 0.03305986151099205, dice: 0\n",
      "#Val#  epoch: 95, dice: 0\n",
      "#Test#  epoch: 95, dice: 0\n",
      "#Train#  epoch: 96, loss: 0.029728014022111893, dice: 0\n",
      "#Val#  epoch: 96, dice: 0\n",
      "#Test#  epoch: 96, dice: 0\n",
      "#Train#  epoch: 97, loss: 0.02872299775481224, dice: 0\n",
      "#Val#  epoch: 97, dice: 0\n",
      "#Test#  epoch: 97, dice: 0\n",
      "#Train#  epoch: 98, loss: 0.029351867735385895, dice: 0\n",
      "#Val#  epoch: 98, dice: 0\n",
      "#Test#  epoch: 98, dice: 0\n",
      "#Train#  epoch: 99, loss: 0.031926482915878296, dice: 0\n",
      "#Val#  epoch: 99, dice: 0\n",
      "#Test#  epoch: 99, dice: 0\n",
      "#Train#  epoch: 100, loss: 0.03256680443882942, dice: 0\n",
      "#Val#  epoch: 100, dice: 0\n",
      "#Test#  epoch: 100, dice: 0\n"
     ]
    }
   ],
   "source": [
    "print('#----------Start training----------#')\n",
    "torch.cuda.empty_cache()\n",
    "info = \"%d-resizeh, %d-resizew, %f-outer_lr\"%(config.resize_h,config.resize_w,config.outer_lr)\n",
    "print(info)\n",
    "logger.info(info)\n",
    "best_dice_val = 0.0\n",
    "best_dice_test = 0.0\n",
    "train_csv = os.path.join(csv_save,\"train.csv\")\n",
    "val_csv = os.path.join(csv_save,\"val.csv\")\n",
    "test_csv = os.path.join(csv_save,\"test.csv\")\n",
    "train_columns = ['Epoch','Loss',\"Mdice\"]\n",
    "train_df = pd.DataFrame(columns=train_columns)\n",
    "val_columns = ['Epoch','Mdice']\n",
    "val_df = pd.DataFrame(columns=val_columns)\n",
    "test_columns = ['Epoch','Mdice']\n",
    "test_df = pd.DataFrame(columns=test_columns)\n",
    "for epoch in range(start_epoch, config.epoch_num+1):\n",
    "    # train part\n",
    "    torch.cuda.empty_cache()\n",
    "    predicted_list = []\n",
    "    groundtruth_list = []\n",
    "    loss_list = []    \n",
    "    base_net.train()\n",
    "    for image,mask in train_loader:\n",
    "        # claer the meta_optimizer, setting zero\n",
    "        meta_optimizer.zero_grad()\n",
    "        image = image.to(device)\n",
    "        mask = mask.to(device)\n",
    "        image = torch.squeeze(image,dim=1)      # torch.Size([bs, 3, 512, 512])\n",
    "        mask = torch.squeeze(mask,dim=1)     # torch.Size([bs, 1, 512, 512])\n",
    "        mask = torch.squeeze(mask,dim=1).float()     # torch.Size([bs, 512, 512])\n",
    "        predicted = base_net(image)     # torch.Size([bs,out_channels=1,512,512])\n",
    "        predicted = predicted.squeeze(1)    # torch.Size([bs,512,512])\n",
    "        loss = criterion(predicted,mask)\n",
    "        loss.backward()\n",
    "        meta_optimizer.step()\n",
    "        loss_list.append(loss.cpu().detach().numpy())\n",
    "        predicted = (predicted > threshold).long()\n",
    "        temp_predicted = predicted.cpu().detach().numpy()       # threshold alternative\n",
    "        predicted_list.append(temp_predicted)\n",
    "        groundtruth_list.append(mask.long().cpu().detach().numpy())\n",
    "    # train_dice,train_mdice = evaluation_epoch(predicted_list,groundtruth_list)\n",
    "    train_dice = evaluation_api(predicted_list,groundtruth_list)\n",
    "    train_mloss = np.mean(loss_list)\n",
    "    log_train = f'epoch: {epoch}, loss: {train_mloss}, dice: {train_dice}'\n",
    "    print(\"#Train# \",log_train)\n",
    "    temp_result = pd.Series([epoch,train_mloss,train_dice],index=train_columns)\n",
    "    train_df = train_df.append(temp_result, ignore_index=True)\n",
    "    train_df.to_csv(train_csv, index=False)\n",
    "    \n",
    "    # validation part\n",
    "    torch.cuda.empty_cache()\n",
    "    predicted_list = []\n",
    "    groundtruth_list = []\n",
    "    base_net.eval()\n",
    "    with torch.no_grad():\n",
    "        for image,mask in val_loader:\n",
    "            # claer the meta_optimizer, setting zero\n",
    "            meta_optimizer.zero_grad()\n",
    "            image = image.to(device)\n",
    "            mask = mask.to(device)\n",
    "            image = torch.squeeze(image,dim=1)      # torch.Size([bs, 3, 512, 512])\n",
    "            mask = torch.squeeze(mask,dim=1)     # torch.Size([bs, 1, 512, 512])\n",
    "            mask = torch.squeeze(mask,dim=1).float()     # torch.Size([bs, 512, 512])\n",
    "            predicted = base_net(image)\n",
    "            temp_predicted = (predicted > threshold).long().cpu().detach().numpy()        # (20, 128, 128)\n",
    "            predicted_list.append(temp_predicted)\n",
    "            groundtruth_list.append(mask.long().cpu().detach().numpy())\n",
    "        # val_dice,val_mdice = evaluation_epoch(predicted_list,groundtruth_list)\n",
    "        val_dice = evaluation_api(predicted_list,groundtruth_list)\n",
    "        log_val = f'epoch: {epoch}, dice: {val_dice}'\n",
    "        print(\"#Val# \",log_val)\n",
    "        temp_result = pd.Series([epoch,val_dice],index=val_columns)\n",
    "        val_df = val_df.append(temp_result, ignore_index=True)\n",
    "        val_df.to_csv(val_csv, index=False)\n",
    "        # logger.info(log_val)\n",
    "\n",
    "    if val_dice > best_dice_val:\n",
    "        torch.save(base_net.state_dict(), os.path.join(checkpoint_dir, 'best_val.pth'))\n",
    "        best_dice_val = val_dice\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # test part\n",
    "    torch.cuda.empty_cache()\n",
    "    predicted_list = []\n",
    "    groundtruth_list = []\n",
    "    base_net.eval()\n",
    "    with torch.no_grad():\n",
    "        for image,mask in test_loader:\n",
    "            # claer the meta_optimizer, setting zero\n",
    "            meta_optimizer.zero_grad()\n",
    "            image = image.to(device)\n",
    "            mask = mask.to(device)\n",
    "            image = torch.squeeze(image,dim=1)      # torch.Size([bs, 3, 512, 512])\n",
    "            mask = torch.squeeze(mask,dim=1)     # torch.Size([bs, 1, 512, 512])\n",
    "            mask = torch.squeeze(mask,dim=1).float()     # torch.Size([bs, 512, 512])\n",
    "            predicted = base_net(image)\n",
    "            temp_predicted = (predicted > threshold).long().cpu().detach().numpy()        # (20, 128, 128)\n",
    "            predicted_list.append(temp_predicted)\n",
    "            groundtruth_list.append(mask.long().cpu().detach().numpy())\n",
    "        # test_dice,test_mdice = evaluation_epoch(predicted_list,groundtruth_list)\n",
    "        test_dice = evaluation_api(predicted_list,groundtruth_list)\n",
    "        log_test = f'epoch: {epoch}, dice: {test_dice}'\n",
    "        print(\"#Test# \",log_test)\n",
    "        temp_result = pd.Series([epoch,test_dice],index=test_columns)\n",
    "        test_df = test_df.append(temp_result, ignore_index=True)\n",
    "        test_df.to_csv(test_csv, index=False)\n",
    "        logger.info(log_test)\n",
    "\n",
    "    if test_dice > best_dice_test:\n",
    "        torch.save(base_net.state_dict(), os.path.join(checkpoint_dir, 'best_test.pth'))\n",
    "        best_dice_test = test_dice\n",
    "    torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best dice in testset:0.0\n"
     ]
    }
   ],
   "source": [
    "best_result_test = \"Best dice in testset:\" + str(best_dice_test)\n",
    "print(best_result_test)\n",
    "logger.info(best_result_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataEngineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
