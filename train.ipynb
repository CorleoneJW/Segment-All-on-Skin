{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangjie/anaconda3/envs/dataEngineering/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets.dataset import HAM_datasets\n",
    "from models.meta import Meta\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import timm\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from models.basenet import *\n",
    "from utils import *\n",
    "from configs.config_setting import setting_config\n",
    "from copy import deepcopy\n",
    "import sklearn.metrics as metrics\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.nn.init as init\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "config = setting_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(batch):\n",
    "    support_images = batch['support_images'].squeeze(0)\n",
    "    support_masks = batch['support_masks'].squeeze(0)\n",
    "    query_images = batch['query_images'].squeeze(0)\n",
    "    query_masks = batch['query_masks'].squeeze(0)\n",
    "    return support_images, support_masks, query_images, query_masks\n",
    "\n",
    "# the function of copying the images\n",
    "def copy_file_to_folder(source_file, dest_folder):\n",
    "    if not os.path.exists(dest_folder):\n",
    "        os.makedirs(dest_folder)\n",
    "\n",
    "    dest_path = os.path.join(dest_folder, os.path.basename(source_file))\n",
    "    shutil.copy(source_file, dest_path)\n",
    "\n",
    "def evaluation_basenet(base_net,query_images,query_masks,criterion):\n",
    "        predicted = base_net(query_images)\n",
    "        np.set_printoptions(threshold=np.inf)\n",
    "        loss = criterion(predicted,query_masks)\n",
    "        predicted = torch.argmax(predicted,dim=1).long()\n",
    "        predict_numpy = predicted.detach().cpu().numpy().reshape(-1)\n",
    "        masks_numpy = query_masks.long().detach().cpu().numpy().reshape(-1)\n",
    "        accuracy = metrics.accuracy_score(masks_numpy,predict_numpy)\n",
    "        f1_score = metrics.f1_score(masks_numpy,predict_numpy,average=None)\n",
    "        return accuracy,f1_score,loss\n",
    "\n",
    "def initialize_weights_he(model):\n",
    "    for param in model.parameters():\n",
    "        init.kaiming_uniform_(param, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "def initialize_weights_xavier(model):\n",
    "    for param in model.parameters():\n",
    "        init.xavier_uniform_(param)\n",
    "\n",
    "def initialize_weights_normal(model):\n",
    "    for param in model.parameters():\n",
    "        init.normal_(param, mean=0, std=1)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------Generating data----------#\n"
     ]
    }
   ],
   "source": [
    "print('#----------Generating data----------#')\n",
    "images_resources_path = \"./data/HAM10000/origin/images/\"         # the resource folder of images\n",
    "masks_resources_path = \"./data/HAM10000/origin/masks/\"           # the resource folder of masks\n",
    "ratio = 0.8     # the dataset and testset ratio\n",
    "categories = config.categories\n",
    "categories_dictionary = {}\n",
    "category_id = 1\n",
    "# prepare the csv for groundtruth\n",
    "origin_groundtruth_csv = \"./data/HAM10000/origin/groundtruth/HAM10000_groundtruth.csv\"   # read the csv file\n",
    "origin_groundtruth = pd.read_csv(origin_groundtruth_csv)    # read the csv file of groundtruth\n",
    "\n",
    "# generating the folders for each category in train folder and test folder\n",
    "# create folders for each categories\n",
    "trainset_images_path = \"./data/HAM10000/train/images/\"     # the images path for train dataset\n",
    "trainset_masks_path = \"./data/HAM10000/train/masks/\"     # the masks path for train dataset\n",
    "testset_images_path = \"./data/HAM10000/test/images/\"     # the images path for test dataset\n",
    "testset_masks_path = \"./data/HAM10000/test/masks/\"      # the masks path for test dataset\n",
    "\n",
    "for category in categories:\n",
    "    # prepare the address for folders\n",
    "    category_images_train_path = os.path.join(trainset_images_path,category)\n",
    "    category_masks_train_path = os.path.join(trainset_masks_path,category)\n",
    "    category_images_test_path = os.path.join(testset_images_path,category)\n",
    "    category_masks_test_path = os.path.join(testset_masks_path,category)\n",
    "    #delete the previously exsited folders\n",
    "    shutil.rmtree(category_images_train_path)\n",
    "    shutil.rmtree(category_masks_train_path)\n",
    "    shutil.rmtree(category_images_test_path)\n",
    "    shutil.rmtree(category_masks_test_path)\n",
    "    # create corresponding folder for each categories\n",
    "    os.makedirs(category_images_train_path, exist_ok=True)\n",
    "    os.makedirs(category_masks_train_path, exist_ok=True)\n",
    "    os.makedirs(category_images_test_path, exist_ok=True)\n",
    "    os.makedirs(category_masks_test_path, exist_ok=True)\n",
    "\n",
    "    # generate the data in trainset and testset for each categories\n",
    "    dest_folder_images = \"./data/HAM10000/train/images/\"+category    # the destination train set folder of copying the images\n",
    "    dest_folder_masks = \"./data/HAM10000/train/masks/\"+category    # the destination trian set folder of copying the masks\n",
    "    dest_folder_images_change = \"./data/HAM10000/test/images/\"+category     # the destination folder of test set images\n",
    "    dest_folder_masks_change = \"./data/HAM10000/test/masks/\"+category      # the destination folder of test set masks\n",
    "    data_categories = origin_groundtruth[origin_groundtruth['dx'] == category]      # extract each categories \n",
    "    length_categories = len(data_categories)\n",
    "    chaneg_folder_point = math.floor(length_categories * ratio)     # get the point to change directory name \n",
    "    elements_count = 0\n",
    "    for image_name in data_categories['image_id']:      # each image_id in each categories\n",
    "        if elements_count == chaneg_folder_point:\n",
    "            dest_folder_images = dest_folder_images_change\n",
    "            dest_folder_masks = dest_folder_masks_change\n",
    "        images_file = image_name+\".jpg\"\n",
    "        masks_file = image_name+\"_segmentation.png\"\n",
    "        source_image = images_resources_path+images_file    # the full path of source of image : path + image file name\n",
    "        source_mask = masks_resources_path+masks_file       # the full path of source of mask : path + mask file name\n",
    "        copy_file_to_folder(source_image,dest_folder_images)\n",
    "        # masks should be preprocess to the form of output for network (Width*Height*Category)\n",
    "        image = Image.open(source_mask)\n",
    "        image_array = np.array(image)\n",
    "        image_array[image_array == 255] = category_id\n",
    "        image = Image.fromarray(image_array)\n",
    "        image.save(os.path.join(dest_folder_masks, masks_file))\n",
    "        elements_count +=1\n",
    "    categories_dictionary[category] = category_id       # add the category id in the categories_dictionary\n",
    "    category_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------GPU init----------#\n"
     ]
    }
   ],
   "source": [
    "print('#----------GPU init----------#')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu_id\n",
    "set_seed(config.seed)\n",
    "device = torch.device('cuda')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------Prepareing Model----------#\n"
     ]
    }
   ],
   "source": [
    "print('#----------Prepareing Model----------#')\n",
    "in_channels = config.in_channels\n",
    "out_channels = config.num_classes\n",
    "base_net = UnetPlusPlus(in_channels,out_channels)\n",
    "initialize_weights_normal(base_net)\n",
    "base_net = base_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------Prepareing loss, opt, sch and amp----------#\n"
     ]
    }
   ],
   "source": [
    "print('#----------Prepareing loss, opt, sch and amp----------#')\n",
    "criterion = config.criterion\n",
    "meta_optimizer = get_optimizer(config, base_net)\n",
    "meta_scheduler = get_scheduler(config, meta_optimizer)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------Set other params----------#\n"
     ]
    }
   ],
   "source": [
    "print('#----------Set other params----------#')\n",
    "min_loss = 999\n",
    "start_epoch = 1\n",
    "min_epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------Start training----------#\n",
      "batch_size:1, 3-way, 5-shot, 5-query, 128-resizeh, 128-resizew\n",
      "Evaluation: Epoch: 1, loss: 4.171700772035182e+31, accuracy: 0.1127, f1_score: [0.04315239 0.09192558 0.12952847 0.20611817]\n",
      "Evaluation: Epoch: 2, loss: 3.950440267107345e+31, accuracy: 0.1133, f1_score: [0.04291876 0.16756642 0.15527153 0.1339893 ]\n",
      "Evaluation: Epoch: 3, loss: 4.268528960201724e+31, accuracy: 0.1090, f1_score: [0.04421623 0.08843112 0.18301145 0.14360972]\n",
      "Evaluation: Epoch: 4, loss: 4.221965006192791e+31, accuracy: 0.1065, f1_score: [0.04352051 0.1152186  0.17806737 0.12525463]\n",
      "Evaluation: Epoch: 5, loss: 3.9911300505587704e+31, accuracy: 0.1318, f1_score: [0.0439204  0.10793336 0.22138649 0.16760767]\n",
      "Evaluation: Epoch: 6, loss: 4.093725056670038e+31, accuracy: 0.1128, f1_score: [0.0441927  0.10871493 0.12816848 0.1980218 ]\n",
      "Evaluation: Epoch: 7, loss: 3.8919550955911925e+31, accuracy: 0.1186, f1_score: [0.04102264 0.14728115 0.17561586 0.15210372]\n",
      "Evaluation: Epoch: 8, loss: 3.997609651166741e+31, accuracy: 0.1160, f1_score: [0.04311922 0.14693418 0.13668117 0.1785783 ]\n",
      "Evaluation: Epoch: 9, loss: 3.9683178621278063e+31, accuracy: 0.1229, f1_score: [0.04280269 0.12827793 0.17828655 0.17547077]\n",
      "Evaluation: Epoch: 10, loss: 3.7458573774314203e+31, accuracy: 0.1396, f1_score: [0.04370342 0.13217277 0.15864098 0.23797531]\n",
      "Evaluation: Epoch: 11, loss: 3.691107786697877e+31, accuracy: 0.1371, f1_score: [0.04058824 0.16767799 0.18904528 0.18322944]\n",
      "Evaluation: Epoch: 12, loss: 3.7664707715816693e+31, accuracy: 0.1318, f1_score: [0.04158617 0.16504228 0.11732584 0.22922261]\n",
      "Evaluation: Epoch: 13, loss: 3.796006521851002e+31, accuracy: 0.1390, f1_score: [0.04342461 0.1555768  0.18396581 0.20054932]\n",
      "Evaluation: Epoch: 14, loss: 4.0253421676835365e+31, accuracy: 0.1147, f1_score: [0.04268678 0.17205095 0.11242455 0.17521543]\n",
      "Evaluation: Epoch: 15, loss: 4.07289478122775e+31, accuracy: 0.1296, f1_score: [0.04401065 0.1158842  0.21059467 0.16858961]\n",
      "Evaluation: Epoch: 16, loss: 3.74553870458537e+31, accuracy: 0.1314, f1_score: [0.04194082 0.18018179 0.18637547 0.15359564]\n",
      "Evaluation: Epoch: 17, loss: 3.747150202702916e+31, accuracy: 0.1442, f1_score: [0.04421542 0.14465823 0.19731249 0.21046067]\n",
      "Evaluation: Epoch: 18, loss: 3.863055240302977e+31, accuracy: 0.1477, f1_score: [0.04306876 0.14055091 0.20306858 0.2185429 ]\n",
      "Evaluation: Epoch: 19, loss: 4.065221487265492e+31, accuracy: 0.1130, f1_score: [0.04321779 0.16906668 0.16147111 0.12419723]\n",
      "Evaluation: Epoch: 20, loss: 4.292977792407282e+31, accuracy: 0.1119, f1_score: [0.04306983 0.1119392  0.14737916 0.17845804]\n",
      "Evaluation: Epoch: 21, loss: 3.8344849381931883e+31, accuracy: 0.1248, f1_score: [0.04361807 0.15303337 0.17821253 0.16357701]\n",
      "Evaluation: Epoch: 22, loss: 4.213533473956471e+31, accuracy: 0.1025, f1_score: [0.04256049 0.11601411 0.12309895 0.16696305]\n",
      "Evaluation: Epoch: 23, loss: 4.404318409682806e+31, accuracy: 0.0965, f1_score: [0.0444899  0.16547762 0.07611482 0.14562485]\n",
      "Evaluation: Epoch: 24, loss: 3.8220774907213195e+31, accuracy: 0.1361, f1_score: [0.04193844 0.12725401 0.14620694 0.24202452]\n",
      "Evaluation: Epoch: 25, loss: 4.29150338647768e+31, accuracy: 0.1054, f1_score: [0.04357751 0.11882816 0.1043393  0.18869349]\n",
      "Evaluation: Epoch: 26, loss: 4.145504316664788e+31, accuracy: 0.1080, f1_score: [0.04395183 0.12200108 0.16604327 0.13924202]\n",
      "Evaluation: Epoch: 27, loss: 4.270938107575052e+31, accuracy: 0.1067, f1_score: [0.04280006 0.12687924 0.12472376 0.17255149]\n",
      "Evaluation: Epoch: 28, loss: 3.5945445950709944e+31, accuracy: 0.1330, f1_score: [0.04239907 0.19167147 0.10166246 0.21982254]\n",
      "Evaluation: Epoch: 29, loss: 4.063867490347524e+31, accuracy: 0.1220, f1_score: [0.04309274 0.11893683 0.18185464 0.1735367 ]\n",
      "Evaluation: Epoch: 30, loss: 4.168326418287473e+31, accuracy: 0.1158, f1_score: [0.04546857 0.061246   0.24729494 0.09467035]\n",
      "Evaluation: Epoch: 31, loss: 4.420586682652196e+31, accuracy: 0.0978, f1_score: [0.0443283  0.09876581 0.11134195 0.16854179]\n",
      "Evaluation: Epoch: 32, loss: 3.704101321407095e+31, accuracy: 0.1408, f1_score: [0.0405393  0.16263009 0.18654657 0.20000925]\n",
      "Evaluation: Epoch: 33, loss: 4.443286440981936e+31, accuracy: 0.0940, f1_score: [0.04435973 0.12921504 0.14562181 0.1016197 ]\n",
      "Evaluation: Epoch: 34, loss: 3.972474874451133e+31, accuracy: 0.1216, f1_score: [0.04257628 0.12380547 0.15984435 0.19150898]\n",
      "Evaluation: Epoch: 35, loss: 3.8830339483979283e+31, accuracy: 0.1212, f1_score: [0.04325541 0.11106153 0.1703614  0.18675567]\n",
      "Evaluation: Epoch: 36, loss: 3.910333911256466e+31, accuracy: 0.1429, f1_score: [0.04238916 0.14347457 0.14414584 0.25208062]\n",
      "Evaluation: Epoch: 37, loss: 4.45579156966003e+31, accuracy: 0.0910, f1_score: [0.04445834 0.11447211 0.11677245 0.13068647]\n",
      "Evaluation: Epoch: 38, loss: 3.854648128368213e+31, accuracy: 0.1333, f1_score: [0.04193468 0.15820537 0.16179242 0.20334616]\n",
      "Evaluation: Epoch: 39, loss: 4.221914714878695e+31, accuracy: 0.1130, f1_score: [0.04362405 0.11856018 0.14152246 0.18235472]\n",
      "Evaluation: Epoch: 40, loss: 3.9149343575704274e+31, accuracy: 0.1334, f1_score: [0.04214046 0.15435458 0.14337322 0.22060149]\n",
      "Evaluation: Epoch: 41, loss: 3.9668151673340253e+31, accuracy: 0.1298, f1_score: [0.04384611 0.09125463 0.19266084 0.20081432]\n",
      "Evaluation: Epoch: 42, loss: 4.201653118141954e+31, accuracy: 0.1115, f1_score: [0.04375018 0.1384932  0.08474149 0.20988839]\n",
      "Evaluation: Epoch: 43, loss: 4.098389092482111e+31, accuracy: 0.1194, f1_score: [0.04277805 0.14953319 0.17026142 0.1563345 ]\n",
      "Evaluation: Epoch: 44, loss: 4.579436633927591e+31, accuracy: 0.0835, f1_score: [0.04406944 0.07982701 0.12636456 0.11360899]\n",
      "Evaluation: Epoch: 45, loss: 3.791333056417536e+31, accuracy: 0.1199, f1_score: [0.0422527  0.14320588 0.16449801 0.16953145]\n",
      "Evaluation: Epoch: 46, loss: 4.13400356355763e+31, accuracy: 0.1075, f1_score: [0.04385027 0.14944531 0.11846955 0.16368752]\n",
      "Evaluation: Epoch: 47, loss: 3.8109558567511927e+31, accuracy: 0.1349, f1_score: [0.04357528 0.11932096 0.21490379 0.17964321]\n",
      "Evaluation: Epoch: 48, loss: 3.8150756341592755e+31, accuracy: 0.1322, f1_score: [0.04355952 0.1475106  0.10248569 0.25011721]\n",
      "Evaluation: Epoch: 49, loss: 4.306025003422891e+31, accuracy: 0.0980, f1_score: [0.04355198 0.11045154 0.11362984 0.16222874]\n",
      "Evaluation: Epoch: 50, loss: 3.9227292694701386e+31, accuracy: 0.1219, f1_score: [0.04100699 0.16927769 0.16357609 0.15581725]\n",
      "Evaluation: Epoch: 51, loss: 4.413116971797961e+31, accuracy: 0.0911, f1_score: [0.0438089  0.1075535  0.09279914 0.15710486]\n",
      "Evaluation: Epoch: 52, loss: 4.129327438487361e+31, accuracy: 0.1092, f1_score: [0.0426638  0.17164767 0.11412495 0.155261  ]\n",
      "Evaluation: Epoch: 53, loss: 4.071955204080746e+31, accuracy: 0.1111, f1_score: [0.04280693 0.17210406 0.12664223 0.15088797]\n",
      "Evaluation: Epoch: 54, loss: 3.9291917033314705e+31, accuracy: 0.1120, f1_score: [0.04228892 0.15840953 0.13832377 0.15543577]\n",
      "Evaluation: Epoch: 55, loss: 3.932527855023279e+31, accuracy: 0.1271, f1_score: [0.04242948 0.13570598 0.14750782 0.21189887]\n",
      "Evaluation: Epoch: 56, loss: 4.0507811101353593e+31, accuracy: 0.1260, f1_score: [0.04262825 0.13433946 0.14126235 0.21398375]\n",
      "Evaluation: Epoch: 57, loss: 3.4067730278426306e+31, accuracy: 0.1434, f1_score: [0.04242786 0.17759563 0.21100175 0.16895583]\n",
      "Evaluation: Epoch: 58, loss: 3.9770068955637044e+31, accuracy: 0.1101, f1_score: [0.04239372 0.14954685 0.1473925  0.14671906]\n",
      "Evaluation: Epoch: 59, loss: 4.0074713426476652e+31, accuracy: 0.1119, f1_score: [0.04343246 0.16376845 0.12261783 0.16321882]\n",
      "Evaluation: Epoch: 60, loss: 4.118420509742798e+31, accuracy: 0.1118, f1_score: [0.043497   0.15050562 0.12109952 0.17607521]\n",
      "Evaluation: Epoch: 61, loss: 4.045160330429643e+31, accuracy: 0.1217, f1_score: [0.04318778 0.12041352 0.12934759 0.21753273]\n",
      "Evaluation: Epoch: 62, loss: 4.46068530137783e+31, accuracy: 0.0842, f1_score: [0.04441516 0.16403921 0.05034635 0.12234182]\n",
      "Evaluation: Epoch: 63, loss: 4.075872607306625e+31, accuracy: 0.1225, f1_score: [0.0435667  0.08674605 0.1779773  0.19521236]\n",
      "Evaluation: Epoch: 64, loss: 3.993192719792197e+31, accuracy: 0.1266, f1_score: [0.04403745 0.09026971 0.16097204 0.22019675]\n",
      "Evaluation: Epoch: 65, loss: 4.016864454480907e+31, accuracy: 0.1172, f1_score: [0.04317732 0.13931194 0.13773948 0.18657105]\n",
      "Evaluation: Epoch: 66, loss: 4.202429732088474e+31, accuracy: 0.1196, f1_score: [0.04452598 0.10540844 0.20953053 0.14133915]\n",
      "Evaluation: Epoch: 67, loss: 3.862897838161263e+31, accuracy: 0.1296, f1_score: [0.0425101  0.13236778 0.2326325  0.13142665]\n",
      "Evaluation: Epoch: 68, loss: 4.012858557885032e+31, accuracy: 0.1100, f1_score: [0.04188185 0.16473386 0.09836276 0.1771555 ]\n",
      "Evaluation: Epoch: 69, loss: 4.0260007904700626e+31, accuracy: 0.1201, f1_score: [0.0429381  0.18132014 0.1515754  0.1482935 ]\n",
      "Evaluation: Epoch: 70, loss: 4.120726656636295e+31, accuracy: 0.1055, f1_score: [0.04293964 0.12724685 0.12761257 0.16588678]\n",
      "Evaluation: Epoch: 71, loss: 3.730602667869195e+31, accuracy: 0.1322, f1_score: [0.04159156 0.16983162 0.16095107 0.19220099]\n",
      "Evaluation: Epoch: 72, loss: 4.602795015043857e+31, accuracy: 0.0823, f1_score: [0.04667976 0.12102567 0.09553718 0.10891635]\n",
      "Evaluation: Epoch: 73, loss: 4.045443219071433e+31, accuracy: 0.1033, f1_score: [0.04237863 0.13631901 0.10771641 0.16973769]\n",
      "Evaluation: Epoch: 74, loss: 4.840553005276483e+31, accuracy: 0.0628, f1_score: [0.04561343 0.10355482 0.06255921 0.07270785]\n",
      "Evaluation: Epoch: 75, loss: 4.046116832538122e+31, accuracy: 0.1174, f1_score: [0.04327621 0.13949912 0.19190819 0.13181818]\n",
      "Evaluation: Epoch: 76, loss: 4.015305423743932e+31, accuracy: 0.1228, f1_score: [0.04382351 0.11791911 0.21212997 0.14198958]\n",
      "Evaluation: Epoch: 77, loss: 3.760849750090789e+31, accuracy: 0.1380, f1_score: [0.04278511 0.15412862 0.22322494 0.15661411]\n",
      "Evaluation: Epoch: 78, loss: 4.159946144505905e+31, accuracy: 0.1077, f1_score: [0.04389494 0.11754338 0.14311883 0.16416064]\n",
      "Evaluation: Epoch: 79, loss: 4.270795454328337e+31, accuracy: 0.1018, f1_score: [0.0450208  0.101226   0.15764154 0.13647207]\n",
      "Evaluation: Epoch: 80, loss: 4.2326915632050675e+31, accuracy: 0.1008, f1_score: [0.04324278 0.1312488  0.09468079 0.17488912]\n",
      "Evaluation: Epoch: 81, loss: 3.799774018275249e+31, accuracy: 0.1276, f1_score: [0.04196718 0.17743705 0.20712307 0.11767398]\n",
      "Evaluation: Epoch: 82, loss: 4.405467856352095e+31, accuracy: 0.0927, f1_score: [0.045307   0.13461018 0.13532295 0.10221444]\n",
      "Evaluation: Epoch: 83, loss: 3.159447115103247e+31, accuracy: 0.1700, f1_score: [0.041274   0.18225151 0.2627186  0.18661322]\n",
      "Evaluation: Epoch: 84, loss: 3.956338857966409e+31, accuracy: 0.1263, f1_score: [0.04425958 0.15846499 0.07763831 0.24222591]\n",
      "Evaluation: Epoch: 85, loss: 3.5833073877925125e+31, accuracy: 0.1378, f1_score: [0.04060073 0.16693006 0.16949875 0.20459812]\n",
      "Evaluation: Epoch: 86, loss: 4.293903346014779e+31, accuracy: 0.1011, f1_score: [0.04342799 0.16152545 0.11259626 0.13666231]\n",
      "Evaluation: Epoch: 87, loss: 4.5099437092535195e+31, accuracy: 0.0766, f1_score: [0.04475199 0.14688535 0.05227874 0.10737426]\n",
      "Evaluation: Epoch: 88, loss: 4.581245186953735e+31, accuracy: 0.0840, f1_score: [0.04459318 0.1198504  0.13739638 0.07657176]\n",
      "Evaluation: Epoch: 89, loss: 4.123694327738285e+31, accuracy: 0.1114, f1_score: [0.04328526 0.12779721 0.17521229 0.13811262]\n",
      "Evaluation: Epoch: 90, loss: 3.7764797102274227e+31, accuracy: 0.1223, f1_score: [0.04275449 0.16087654 0.12966406 0.19502754]\n",
      "Evaluation: Epoch: 91, loss: 4.1104125851136705e+31, accuracy: 0.1230, f1_score: [0.04381011 0.06776403 0.17131667 0.21026765]\n",
      "Evaluation: Epoch: 92, loss: 3.88046207960928e+31, accuracy: 0.1211, f1_score: [0.04323239 0.15883354 0.18714084 0.13621282]\n",
      "Evaluation: Epoch: 93, loss: 4.077214031396069e+31, accuracy: 0.1196, f1_score: [0.0427074  0.15485783 0.11670979 0.20076327]\n",
      "Evaluation: Epoch: 94, loss: 4.076524943678889e+31, accuracy: 0.1198, f1_score: [0.04317999 0.07281062 0.14864785 0.2184135 ]\n",
      "Evaluation: Epoch: 95, loss: 3.9211956261753755e+31, accuracy: 0.1063, f1_score: [0.04317544 0.1428647  0.15014797 0.13543976]\n",
      "Evaluation: Epoch: 96, loss: 3.8139462556585915e+31, accuracy: 0.1423, f1_score: [0.04112166 0.12382185 0.1855514  0.22947722]\n",
      "Evaluation: Epoch: 97, loss: 4.1614650389056685e+31, accuracy: 0.1098, f1_score: [0.0439706  0.14516698 0.18981452 0.10034187]\n",
      "Evaluation: Epoch: 98, loss: 3.450294599133921e+31, accuracy: 0.1552, f1_score: [0.0419462  0.14478237 0.20813375 0.23294641]\n",
      "Evaluation: Epoch: 99, loss: 4.103105353889592e+31, accuracy: 0.1165, f1_score: [0.04327979 0.098379   0.18221018 0.1666993 ]\n",
      "Evaluation: Epoch: 100, loss: 3.826332909606363e+31, accuracy: 0.1269, f1_score: [0.04127884 0.13809706 0.13923104 0.21685606]\n"
     ]
    }
   ],
   "source": [
    "print('#----------Start training----------#')\n",
    "print(\n",
    "            \"batch_size:%d, %d-way, %d-shot, %d-query, %d-resizeh, %d-resizew\"\n",
    "            % (\n",
    "                config.batch_size,\n",
    "                config.n_way,\n",
    "                config.k_shot,\n",
    "                config.k_query,\n",
    "                config.resize_h,\n",
    "                config.resize_w\n",
    "            )\n",
    "        )\n",
    "torch.cuda.empty_cache()\n",
    "for epoch in range(start_epoch, config.epoch_num+1):\n",
    "    step = 0        # according to the step, decide to print the result or do the evaluation\n",
    "    # create the dataset and dataloader\n",
    "    train_dataset = HAM_datasets(config, train=True)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=config.dataloader_bs, num_workers=config.num_workers)\n",
    "    test_dataset = HAM_datasets(config, train=False)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=config.dataloader_bs, num_workers=config.num_workers)\n",
    "    # numerate to train\n",
    "    for i,batch in enumerate(train_dataset):\n",
    "        # claer the meta_optimizer, setting zero\n",
    "        meta_optimizer.zero_grad()\n",
    "        support_images, support_masks, query_images, query_masks = preprocess_batch(batch)\n",
    "        support_images, support_masks, query_images, query_masks = support_images.to(device), support_masks.to(device), query_images.to(device), query_masks.to(device)\n",
    "        support_masks = torch.squeeze(support_masks,dim=1).long()\n",
    "        query_masks = torch.squeeze(query_masks,dim=1).long()\n",
    "        # deep copy the base_net for inner loop, preparing the loss and optimizer\n",
    "        temp_net = deepcopy(base_net)\n",
    "        inner_optimizer = optim.Adam(temp_net.parameters(), lr=config.inner_lr)\n",
    "        for inner_step in range(config.inner_steps):\n",
    "            inner_optimizer.zero_grad()\n",
    "            predicted = temp_net(support_images)\n",
    "            loss = criterion(predicted,support_masks)\n",
    "            loss.backward()\n",
    "            inner_optimizer.step()\n",
    "        query_predicted = temp_net(query_images)\n",
    "        loss = criterion(query_predicted,query_masks)\n",
    "        loss.backward()\n",
    "        meta_optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "    torch.cuda.empty_cache()\n",
    "    accuracy,f1_score,loss_score = evaluation_basenet(base_net,query_images,query_masks,criterion)\n",
    "    log_info = f'Epoch: {epoch}, loss: {loss_score}, accuracy: {accuracy:.4f}, f1_score: {f1_score}'\n",
    "    print(\"Evaluation:\",log_info)\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataEngineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
