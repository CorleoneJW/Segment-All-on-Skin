{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangjie/anaconda3/envs/dataEngineering/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets.dataset import HAM_datasets\n",
    "from models.meta import Meta\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import timm\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from models.basenet import *\n",
    "from utils import *\n",
    "from configs.config_setting import setting_config\n",
    "from copy import deepcopy\n",
    "import sklearn.metrics as metrics\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.nn.init as init\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "config = setting_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(batch):\n",
    "    support_images = batch['support_images'].squeeze(0)\n",
    "    support_masks = batch['support_masks'].squeeze(0)\n",
    "    query_images = batch['query_images'].squeeze(0)\n",
    "    query_masks = batch['query_masks'].squeeze(0)\n",
    "    return support_images, support_masks, query_images, query_masks\n",
    "\n",
    "# the function of copying the images\n",
    "def copy_file_to_folder(source_file, dest_folder):\n",
    "    if not os.path.exists(dest_folder):\n",
    "        os.makedirs(dest_folder)\n",
    "\n",
    "    dest_path = os.path.join(dest_folder, os.path.basename(source_file))\n",
    "    shutil.copy(source_file, dest_path)\n",
    "\n",
    "def evaluation_basenet(base_net,query_images,query_masks,criterion):\n",
    "        predicted = base_net(query_images)\n",
    "        loss = criterion(predicted,query_masks)\n",
    "        predicted = torch.argmax(predicted,dim=1).long()\n",
    "        predict_numpy = predicted.detach().cpu().numpy().reshape(-1)\n",
    "        masks_numpy = query_masks.long().detach().cpu().numpy().reshape(-1)\n",
    "        accuracy = metrics.accuracy_score(masks_numpy,predict_numpy)\n",
    "        f1_score = metrics.f1_score(masks_numpy,predict_numpy,average=None)\n",
    "        return accuracy,f1_score,loss\n",
    "\n",
    "def initialize_weights_he(model):\n",
    "    for param in model.parameters():\n",
    "        init.kaiming_uniform_(param, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "def initialize_weights_xavier(model):\n",
    "    for param in model.parameters():\n",
    "        init.xavier_uniform_(param)\n",
    "\n",
    "def initialize_weights_normal(model):\n",
    "    for param in model.parameters():\n",
    "        init.normal_(param, mean=0, std=1)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------Generating data----------#\n"
     ]
    }
   ],
   "source": [
    "print('#----------Generating data----------#')\n",
    "images_resources_path = \"./data/HAM10000/origin/images/\"         # the resource folder of images\n",
    "masks_resources_path = \"./data/HAM10000/origin/masks/\"           # the resource folder of masks\n",
    "ratio = 0.8     # the dataset and testset ratio\n",
    "categories = config.categories\n",
    "categories_dictionary = {}\n",
    "category_id = 1\n",
    "# prepare the csv for groundtruth\n",
    "origin_groundtruth_csv = \"./data/HAM10000/origin/groundtruth/HAM10000_groundtruth.csv\"   # read the csv file\n",
    "origin_groundtruth = pd.read_csv(origin_groundtruth_csv)    # read the csv file of groundtruth\n",
    "\n",
    "# generating the folders for each category in train folder and test folder\n",
    "# create folders for each categories\n",
    "trainset_images_path = \"./data/HAM10000/train/images/\"     # the images path for train dataset\n",
    "trainset_masks_path = \"./data/HAM10000/train/masks/\"     # the masks path for train dataset\n",
    "testset_images_path = \"./data/HAM10000/test/images/\"     # the images path for test dataset\n",
    "testset_masks_path = \"./data/HAM10000/test/masks/\"      # the masks path for test dataset\n",
    "\n",
    "for category in categories:\n",
    "    # prepare the address for folders\n",
    "    category_images_train_path = os.path.join(trainset_images_path,category)\n",
    "    category_masks_train_path = os.path.join(trainset_masks_path,category)\n",
    "    category_images_test_path = os.path.join(testset_images_path,category)\n",
    "    category_masks_test_path = os.path.join(testset_masks_path,category)\n",
    "    #delete the previously exsited folders\n",
    "    shutil.rmtree(category_images_train_path)\n",
    "    shutil.rmtree(category_masks_train_path)\n",
    "    shutil.rmtree(category_images_test_path)\n",
    "    shutil.rmtree(category_masks_test_path)\n",
    "    # create corresponding folder for each categories\n",
    "    os.makedirs(category_images_train_path, exist_ok=True)\n",
    "    os.makedirs(category_masks_train_path, exist_ok=True)\n",
    "    os.makedirs(category_images_test_path, exist_ok=True)\n",
    "    os.makedirs(category_masks_test_path, exist_ok=True)\n",
    "\n",
    "    # generate the data in trainset and testset for each categories\n",
    "    dest_folder_images = \"./data/HAM10000/train/images/\"+category    # the destination train set folder of copying the images\n",
    "    dest_folder_masks = \"./data/HAM10000/train/masks/\"+category    # the destination trian set folder of copying the masks\n",
    "    dest_folder_images_change = \"./data/HAM10000/test/images/\"+category     # the destination folder of test set images\n",
    "    dest_folder_masks_change = \"./data/HAM10000/test/masks/\"+category      # the destination folder of test set masks\n",
    "    data_categories = origin_groundtruth[origin_groundtruth['dx'] == category]      # extract each categories \n",
    "    length_categories = len(data_categories)\n",
    "    chaneg_folder_point = math.floor(length_categories * ratio)     # get the point to change directory name \n",
    "    elements_count = 0\n",
    "    for image_name in data_categories['image_id']:      # each image_id in each categories\n",
    "        if elements_count == chaneg_folder_point:\n",
    "            dest_folder_images = dest_folder_images_change\n",
    "            dest_folder_masks = dest_folder_masks_change\n",
    "        images_file = image_name+\".jpg\"\n",
    "        masks_file = image_name+\"_segmentation.png\"\n",
    "        source_image = images_resources_path+images_file    # the full path of source of image : path + image file name\n",
    "        source_mask = masks_resources_path+masks_file       # the full path of source of mask : path + mask file name\n",
    "        copy_file_to_folder(source_image,dest_folder_images)\n",
    "        # masks should be preprocess to the form of output for network (Width*Height*Category)\n",
    "        image = Image.open(source_mask)\n",
    "        image_array = np.array(image)\n",
    "        image_array[image_array == 255] = category_id\n",
    "        image = Image.fromarray(image_array)\n",
    "        image.save(os.path.join(dest_folder_masks, masks_file))\n",
    "        elements_count +=1\n",
    "    categories_dictionary[category] = category_id       # add the category id in the categories_dictionary\n",
    "    category_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------GPU init----------#\n"
     ]
    }
   ],
   "source": [
    "print('#----------GPU init----------#')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu_id\n",
    "set_seed(config.seed)\n",
    "device = torch.device('cuda')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------Prepareing Model----------#\n"
     ]
    }
   ],
   "source": [
    "print('#----------Prepareing Model----------#')\n",
    "in_channels = config.in_channels\n",
    "out_channels = config.num_classes\n",
    "# base_net = smp.Unet(encoder_name='resnet34', encoder_depth=5, encoder_weights=None, decoder_use_batchnorm=True, decoder_channels=(256, 128, 64, 32, 16), decoder_attention_type=None, in_channels=3, classes=config.num_classes, activation=None, aux_params=None)\n",
    "base_net = smp.UnetPlusPlus(encoder_name='resnet34', encoder_depth=5, encoder_weights=None, decoder_use_batchnorm=True, decoder_channels=(256, 128, 64, 32, 16), decoder_attention_type=None, in_channels=3, classes=config.num_classes, activation=None, aux_params=None)\n",
    "# initialize_weights_he(base_net)\n",
    "base_net = base_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------Prepareing loss, opt, sch and amp----------#\n"
     ]
    }
   ],
   "source": [
    "print('#----------Prepareing loss, opt, sch and amp----------#')\n",
    "criterion = config.criterion\n",
    "meta_optimizer = get_optimizer(config, base_net)\n",
    "meta_scheduler = get_scheduler(config, meta_optimizer)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------Set other params----------#\n"
     ]
    }
   ],
   "source": [
    "print('#----------Set other params----------#')\n",
    "min_loss = 999\n",
    "start_epoch = 1\n",
    "min_epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------Start training----------#\n",
      "batch_size:1, 2-way, 10-shot, 10-query, 512-resizeh, 512-resizew, 0.001000-outer_lr,0.000100-inner_lr\n",
      "Evaluation: Epoch: 1, loss: 1.0767360925674438, accuracy: 0.5569, f1_score: [0.72325732 0.11197774 0.00217325 0.25369641]\n",
      "Evaluation: Epoch: 2, loss: 1.167434811592102, accuracy: 0.5273, f1_score: [0.70016476 0.0046251  0.04460136 0.21766368]\n",
      "Evaluation: Epoch: 3, loss: 1.1075576543807983, accuracy: 0.5887, f1_score: [0.77842585 0.22130541 0.0353718  0.        ]\n",
      "Evaluation: Epoch: 4, loss: 0.9778339266777039, accuracy: 0.6344, f1_score: [0.78606105 0.31473684 0.00141438 0.19956007]\n",
      "Evaluation: Epoch: 5, loss: 0.959933876991272, accuracy: 0.6425, f1_score: [0.80121986 0.00234117 0.02216288 0.29004511]\n",
      "Evaluation: Epoch: 6, loss: 0.801529586315155, accuracy: 0.7504, f1_score: [0.8611458  0.49716385 0.00121036 0.26183806]\n",
      "Evaluation: Epoch: 7, loss: 0.9318286180496216, accuracy: 0.7032, f1_score: [0.84639153 0.61833374 0.01392458 0.        ]\n",
      "Evaluation: Epoch: 8, loss: 0.7630422115325928, accuracy: 0.7737, f1_score: [0.87953055 0.60654281 0.00097829 0.33701366]\n",
      "Evaluation: Epoch: 9, loss: 0.7819186449050903, accuracy: 0.7508, f1_score: [0.86948258 0.59092318 0.0014005  0.22096337]\n",
      "Evaluation: Epoch: 10, loss: 0.7987143397331238, accuracy: 0.7360, f1_score: [8.73327954e-01 5.36541848e-01 3.26051516e-04 1.57763140e-01]\n",
      "Evaluation: Epoch: 11, loss: 0.8269602656364441, accuracy: 0.7287, f1_score: [0.8717044  0.52433506 0.00094849 0.25858955]\n",
      "Evaluation: Epoch: 12, loss: 0.9313899874687195, accuracy: 0.6287, f1_score: [0.83538218 0.00211034 0.02272857 0.16604396]\n",
      "Evaluation: Epoch: 13, loss: 1.0068680047988892, accuracy: 0.6619, f1_score: [0.79988543 0.62366348 0.01179079 0.        ]\n",
      "Evaluation: Epoch: 14, loss: 0.7242463827133179, accuracy: 0.7649, f1_score: [0.8795102  0.48747457 0.00102218 0.24958447]\n",
      "Evaluation: Epoch: 15, loss: 0.7903157472610474, accuracy: 0.7449, f1_score: [0.86306934 0.63849158 0.00236726 0.24881164]\n",
      "Evaluation: Epoch: 16, loss: 0.8715953826904297, accuracy: 0.6438, f1_score: [0.8708113  0.00330978 0.01869657 0.15581471]\n",
      "Evaluation: Epoch: 17, loss: 0.7042239904403687, accuracy: 0.7982, f1_score: [0.90737976 0.6740755  0.00107601 0.30748363]\n",
      "Evaluation: Epoch: 18, loss: 0.7192139625549316, accuracy: 0.8011, f1_score: [0.90884215 0.70197128 0.04679778 0.        ]\n",
      "Evaluation: Epoch: 19, loss: 0.8423078656196594, accuracy: 0.6863, f1_score: [0.88362941 0.00434265 0.00542662 0.16962196]\n",
      "Evaluation: Epoch: 20, loss: 0.7352544069290161, accuracy: 0.7992, f1_score: [0.89402342 0.77606283 0.01412697 0.        ]\n",
      "Evaluation: Epoch: 21, loss: 1.008633017539978, accuracy: 0.5726, f1_score: [0.80998023 0.00173578 0.01570881 0.14073373]\n",
      "Evaluation: Epoch: 22, loss: 0.7420057058334351, accuracy: 0.7627, f1_score: [0.88718799 0.61687278 0.0037178  0.16399029]\n",
      "Evaluation: Epoch: 23, loss: 0.8322521448135376, accuracy: 0.7278, f1_score: [0.85158886 0.61221134 0.01999497 0.        ]\n",
      "Evaluation: Epoch: 24, loss: 0.90110844373703, accuracy: 0.6422, f1_score: [0.84280974 0.00278941 0.03139651 0.18016334]\n",
      "Evaluation: Epoch: 25, loss: 0.7139582633972168, accuracy: 0.7718, f1_score: [0.89741527 0.63398166 0.00259838 0.22895372]\n",
      "Evaluation: Epoch: 26, loss: 0.6603108048439026, accuracy: 0.8004, f1_score: [0.91016636 0.63521677 0.00125136 0.21630544]\n",
      "Evaluation: Epoch: 27, loss: 0.8352341651916504, accuracy: 0.6640, f1_score: [0.87260912 0.00437143 0.04389825 0.17082158]\n",
      "Evaluation: Epoch: 28, loss: 0.7188558578491211, accuracy: 0.7608, f1_score: [0.87183982 0.69648294 0.00313591 0.21097325]\n",
      "Evaluation: Epoch: 29, loss: 0.6661925911903381, accuracy: 0.8285, f1_score: [0.91838695 0.6822404  0.07153598 0.        ]\n",
      "Evaluation: Epoch: 30, loss: 0.6247625946998596, accuracy: 0.7992, f1_score: [0.92620442 0.50173879 0.00271656 0.29034886]\n",
      "Evaluation: Epoch: 31, loss: 0.7910532355308533, accuracy: 0.7556, f1_score: [0.87914762 0.66099536 0.06855721 0.        ]\n",
      "Evaluation: Epoch: 32, loss: 0.6964661478996277, accuracy: 0.7762, f1_score: [0.8924694  0.71555129 0.05396927 0.        ]\n",
      "Evaluation: Epoch: 33, loss: 0.9467331171035767, accuracy: 0.6225, f1_score: [0.83841629 0.00230363 0.02335579 0.170977  ]\n",
      "Evaluation: Epoch: 34, loss: 0.6040323972702026, accuracy: 0.8361, f1_score: [0.93838941 0.65289928 0.00585133 0.43193765]\n",
      "Evaluation: Epoch: 35, loss: 0.6058751940727234, accuracy: 0.8227, f1_score: [0.93713801 0.56976387 0.00402237 0.29640615]\n",
      "Evaluation: Epoch: 36, loss: 0.6042044758796692, accuracy: 0.8305, f1_score: [0.9429622  0.58206058 0.00790899 0.46493386]\n",
      "Evaluation: Epoch: 37, loss: 0.7633077502250671, accuracy: 0.7418, f1_score: [0.87480286 0.49288997 0.00403921 0.31532874]\n",
      "Evaluation: Epoch: 38, loss: 0.9283466339111328, accuracy: 0.6767, f1_score: [0.82779018 0.53203608 0.03065092 0.        ]\n",
      "Evaluation: Epoch: 39, loss: 0.8963152170181274, accuracy: 0.6543, f1_score: [0.86431388 0.00185682 0.04246708 0.22678375]\n",
      "Evaluation: Epoch: 40, loss: 0.9303749799728394, accuracy: 0.6557, f1_score: [0.85525419 0.00267412 0.02710925 0.39322764]\n",
      "Evaluation: Epoch: 41, loss: 0.8340118527412415, accuracy: 0.7380, f1_score: [0.87086434 0.60186805 0.01579045 0.        ]\n",
      "Evaluation: Epoch: 42, loss: 0.6704126596450806, accuracy: 0.7973, f1_score: [0.87989292 0.78728496 0.00145033 0.35794351]\n",
      "Evaluation: Epoch: 43, loss: 0.5835930109024048, accuracy: 0.8349, f1_score: [0.91405642 0.75970988 0.00449132 0.41272384]\n",
      "Evaluation: Epoch: 44, loss: 0.8117307424545288, accuracy: 0.6767, f1_score: [0.87091159 0.00300195 0.02141873 0.35405652]\n",
      "Evaluation: Epoch: 45, loss: 0.6819196939468384, accuracy: 0.7640, f1_score: [0.90122786 0.66106564 0.04369973 0.        ]\n",
      "Evaluation: Epoch: 46, loss: 0.6800277233123779, accuracy: 0.7674, f1_score: [0.92246992 0.00398264 0.01526186 0.37560172]\n",
      "Evaluation: Epoch: 47, loss: 0.7671474814414978, accuracy: 0.6963, f1_score: [0.89927292 0.00223009 0.05749568 0.27738265]\n",
      "Evaluation: Epoch: 48, loss: 0.7961251139640808, accuracy: 0.6807, f1_score: [0.85974831 0.00323445 0.0521883  0.33719596]\n",
      "Evaluation: Epoch: 49, loss: 0.8729103803634644, accuracy: 0.6865, f1_score: [0.85335878 0.39509934 0.0643149  0.        ]\n",
      "Evaluation: Epoch: 50, loss: 0.8142274618148804, accuracy: 0.7178, f1_score: [0.88668385 0.54061547 0.05468166 0.        ]\n",
      "Evaluation: Epoch: 51, loss: 0.7912023663520813, accuracy: 0.7097, f1_score: [0.87256908 0.00473965 0.03485211 0.38339338]\n",
      "Evaluation: Epoch: 52, loss: 0.783106803894043, accuracy: 0.7394, f1_score: [0.85590213 0.5848461  0.00552642 0.35615282]\n",
      "Evaluation: Epoch: 53, loss: 0.6725199818611145, accuracy: 0.7793, f1_score: [0.9079728  0.54190268 0.0051873  0.33106762]\n",
      "Evaluation: Epoch: 54, loss: 0.7407529950141907, accuracy: 0.7766, f1_score: [0.90177111 0.68369992 0.17062332 0.        ]\n",
      "Evaluation: Epoch: 55, loss: 0.6624852418899536, accuracy: 0.7910, f1_score: [0.91402001 0.62753892 0.18998404 0.        ]\n",
      "Evaluation: Epoch: 56, loss: 0.8020947575569153, accuracy: 0.6744, f1_score: [0.85766449 0.00249234 0.06119628 0.33249746]\n",
      "Evaluation: Epoch: 57, loss: 0.7678832411766052, accuracy: 0.7446, f1_score: [0.87833883 0.53286554 0.10317427 0.        ]\n",
      "Evaluation: Epoch: 58, loss: 0.8187511563301086, accuracy: 0.7562, f1_score: [0.85314345 0.81879026 0.13433144 0.        ]\n",
      "Evaluation: Epoch: 59, loss: 0.8236175775527954, accuracy: 0.6900, f1_score: [0.87530352 0.00366321 0.11766353 0.31026879]\n",
      "Evaluation: Epoch: 60, loss: 0.8133031725883484, accuracy: 0.6995, f1_score: [0.87212533 0.00414267 0.28402445 0.08730048]\n",
      "Evaluation: Epoch: 61, loss: 0.8272058367729187, accuracy: 0.7270, f1_score: [0.86856045 0.58390356 0.10425282 0.        ]\n",
      "Evaluation: Epoch: 62, loss: 0.8618732690811157, accuracy: 0.6390, f1_score: [0.83430488 0.00217115 0.20553257 0.26776064]\n",
      "Evaluation: Epoch: 63, loss: 0.7370049953460693, accuracy: 0.7046, f1_score: [0.88697029 0.00270072 0.21709358 0.16477327]\n",
      "Evaluation: Epoch: 64, loss: 0.6340113282203674, accuracy: 0.8007, f1_score: [0.92456465 0.55764224 0.00312874 0.31008691]\n",
      "Evaluation: Epoch: 65, loss: 0.6643409729003906, accuracy: 0.7962, f1_score: [0.89742368 0.79867454 0.00124224 0.40702343]\n",
      "Evaluation: Epoch: 66, loss: 0.6325563192367554, accuracy: 0.8304, f1_score: [0.91950806 0.77118276 0.29880776 0.        ]\n",
      "Evaluation: Epoch: 67, loss: 0.7389353513717651, accuracy: 0.7111, f1_score: [0.88962591 0.00220818 0.24885477 0.28299524]\n",
      "Evaluation: Epoch: 68, loss: 0.6470931172370911, accuracy: 0.7876, f1_score: [0.91817927 0.56937924 0.27796357 0.        ]\n",
      "Evaluation: Epoch: 69, loss: 0.7616914510726929, accuracy: 0.7011, f1_score: [0.89752649 0.00199618 0.24073169 0.23948083]\n",
      "Evaluation: Epoch: 70, loss: 0.5606598854064941, accuracy: 0.7985, f1_score: [0.96099655 0.00448397 0.3285193  0.26946646]\n",
      "Evaluation: Epoch: 71, loss: 0.7643729448318481, accuracy: 0.7469, f1_score: [0.87391027 0.54676893 0.00240813 0.40405256]\n",
      "Evaluation: Epoch: 72, loss: 0.6405097246170044, accuracy: 0.7885, f1_score: [0.90884051 0.70785145 0.29937264 0.        ]\n",
      "Evaluation: Epoch: 73, loss: 0.656999945640564, accuracy: 0.7840, f1_score: [0.91430497 0.61768526 0.0050755  0.36292429]\n",
      "Evaluation: Epoch: 74, loss: 0.7534603476524353, accuracy: 0.7006, f1_score: [0.8786313  0.00401432 0.26827915 0.2369378 ]\n",
      "Evaluation: Epoch: 75, loss: 0.6760843992233276, accuracy: 0.7804, f1_score: [0.90528634 0.58466951 0.29829996 0.        ]\n",
      "Evaluation: Epoch: 76, loss: 0.7103877067565918, accuracy: 0.7703, f1_score: [0.88618308 0.70042628 0.00481638 0.39493023]\n",
      "Evaluation: Epoch: 77, loss: 0.8953503370285034, accuracy: 0.6352, f1_score: [0.83573122 0.00282116 0.2759452  0.26205406]\n",
      "Evaluation: Epoch: 78, loss: 0.5812447667121887, accuracy: 0.8247, f1_score: [0.9310789  0.72269567 0.00308393 0.48339016]\n",
      "Evaluation: Epoch: 79, loss: 0.5950073599815369, accuracy: 0.8287, f1_score: [0.93842652 0.67024565 0.00730959 0.54379899]\n",
      "Evaluation: Epoch: 80, loss: 0.690403163433075, accuracy: 0.7366, f1_score: [0.89411389 0.00353222 0.27292799 0.47833063]\n",
      "Evaluation: Epoch: 81, loss: 0.7091630101203918, accuracy: 0.7559, f1_score: [0.89277361 0.54060936 0.358114   0.        ]\n",
      "Evaluation: Epoch: 82, loss: 0.5533908009529114, accuracy: 0.8209, f1_score: [0.93861679 0.81575165 0.29562551 0.        ]\n",
      "Evaluation: Epoch: 83, loss: 0.632958710193634, accuracy: 0.7948, f1_score: [0.92069906 0.62830585 0.00510518 0.4555202 ]\n",
      "Evaluation: Epoch: 84, loss: 0.6530761122703552, accuracy: 0.8018, f1_score: [0.91036414 0.79063405 0.34940638 0.        ]\n",
      "Evaluation: Epoch: 85, loss: 0.670225977897644, accuracy: 0.7520, f1_score: [0.914127   0.00481944 0.28185976 0.50166183]\n",
      "Evaluation: Epoch: 86, loss: 0.519560694694519, accuracy: 0.8583, f1_score: [0.9502526  0.69402258 0.00419626 0.51207011]\n",
      "Evaluation: Epoch: 87, loss: 0.6556398868560791, accuracy: 0.7564, f1_score: [0.91889358 0.00481768 0.18179744 0.50846835]\n",
      "Evaluation: Epoch: 88, loss: 0.6462956666946411, accuracy: 0.7631, f1_score: [0.9126104  0.00516619 0.28571213 0.5352734 ]\n",
      "Evaluation: Epoch: 89, loss: 0.8682637214660645, accuracy: 0.7018, f1_score: [0.85175446 0.61808574 0.17736343 0.        ]\n",
      "Evaluation: Epoch: 90, loss: 0.6624572277069092, accuracy: 0.7934, f1_score: [0.8985198  0.76788172 0.29199746 0.        ]\n",
      "Evaluation: Epoch: 91, loss: 0.5504277944564819, accuracy: 0.8237, f1_score: [0.93948222 0.67873156 0.32307894 0.        ]\n",
      "Evaluation: Epoch: 92, loss: 0.5821110606193542, accuracy: 0.8247, f1_score: [0.93122464 0.76867886 0.31455062 0.        ]\n",
      "Evaluation: Epoch: 93, loss: 0.728814959526062, accuracy: 0.7243, f1_score: [0.90068787 0.00226097 0.28604048 0.31473972]\n",
      "Evaluation: Epoch: 94, loss: 0.5561257600784302, accuracy: 0.8360, f1_score: [0.94401561 0.66583126 0.00275271 0.4149141 ]\n",
      "Evaluation: Epoch: 95, loss: 0.5930091142654419, accuracy: 0.8070, f1_score: [0.93526325 0.63309059 0.00148244 0.38729937]\n",
      "Evaluation: Epoch: 96, loss: 0.7175479531288147, accuracy: 0.7051, f1_score: [0.90109306 0.00327197 0.28797614 0.22459742]\n",
      "Evaluation: Epoch: 97, loss: 1.0188405513763428, accuracy: 0.6259, f1_score: [0.79112979 0.00407867 0.33330626 0.12774345]\n",
      "Evaluation: Epoch: 98, loss: 0.51716548204422, accuracy: 0.8443, f1_score: [0.94708907 0.75066319 0.40348772 0.        ]\n",
      "Evaluation: Epoch: 99, loss: 0.6761635541915894, accuracy: 0.7306, f1_score: [0.89833272 0.00172593 0.41794404 0.37074201]\n",
      "Evaluation: Epoch: 100, loss: 0.4856012463569641, accuracy: 0.8734, f1_score: [0.95283747 0.80302425 0.50399202 0.        ]\n"
     ]
    }
   ],
   "source": [
    "print('#----------Start training----------#')\n",
    "print(\n",
    "            \"batch_size:%d, %d-way, %d-shot, %d-query, %d-resizeh, %d-resizew, %f-outer_lr,%f-inner_lr\"\n",
    "            % (\n",
    "                config.batch_size,\n",
    "                config.n_way,\n",
    "                config.k_shot,\n",
    "                config.k_query,\n",
    "                config.resize_h,\n",
    "                config.resize_w,\n",
    "                config.inner_lr,\n",
    "                config.outer_lr\n",
    "            )\n",
    "        )\n",
    "torch.cuda.empty_cache()\n",
    "for epoch in range(start_epoch, config.epoch_num+1):\n",
    "    step = 0        # according to the step, decide to print the result or do the evaluation\n",
    "    # create the dataset and dataloader\n",
    "    train_dataset = HAM_datasets(config, train=True)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=config.dataloader_bs, num_workers=config.num_workers)\n",
    "    test_dataset = HAM_datasets(config, train=False)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=config.dataloader_bs, num_workers=config.num_workers)\n",
    "    # numerate to train\n",
    "    for i,batch in enumerate(train_dataset):\n",
    "        # claer the meta_optimizer, setting zero\n",
    "        meta_optimizer.zero_grad()\n",
    "        support_images, support_masks, query_images, query_masks = preprocess_batch(batch)\n",
    "        support_images, support_masks, query_images, query_masks = support_images.to(device), support_masks.to(device), query_images.to(device), query_masks.to(device)\n",
    "        support_masks = torch.squeeze(support_masks,dim=1).long()\n",
    "        query_masks = torch.squeeze(query_masks,dim=1).long()\n",
    "        # deep copy the base_net for inner loop, preparing the loss and optimizer\n",
    "        temp_net = deepcopy(base_net)\n",
    "        inner_optimizer = optim.SGD(temp_net.parameters(), lr=config.inner_lr)\n",
    "        temp_loss = None\n",
    "        for inner_step in range(config.inner_steps):\n",
    "            inner_optimizer.zero_grad()\n",
    "            predicted = temp_net(support_images)\n",
    "            loss = criterion(predicted,support_masks)\n",
    "            loss.backward()\n",
    "            inner_optimizer.step()\n",
    "        query_predicted = temp_net(query_images)\n",
    "        meta_loss = criterion(query_predicted,query_masks)\n",
    "        meta_loss.backward()\n",
    "        with torch.no_grad():\n",
    "                for param1, param2 in zip(temp_net.parameters(), base_net.parameters()):\n",
    "                    if param1.grad is not None:\n",
    "                        param2.grad = param1.grad.clone()\n",
    "        meta_optimizer.step()\n",
    "    with torch.no_grad():\n",
    "        accuracy,f1_score,loss_score = evaluation_basenet(base_net,query_images,query_masks,criterion)\n",
    "        log_info = f'Epoch: {epoch}, loss: {loss_score}, accuracy: {accuracy:.4f}, f1_score: {f1_score}'\n",
    "        print(\"Evaluation:\",log_info)\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataEngineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
