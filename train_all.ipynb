{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangjie/anaconda3/envs/dataEngineering/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets.dataset import HAM_datasets\n",
    "from models.meta import Meta\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import timm\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from models.basenet import *\n",
    "from utils import *\n",
    "from configs.config_setting import setting_config\n",
    "from copy import deepcopy\n",
    "import sklearn.metrics as metrics\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.nn.init as init\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "config = setting_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(batch):\n",
    "    support_images = batch['support_images'].squeeze(0)\n",
    "    support_masks = batch['support_masks'].squeeze(0)\n",
    "    query_images = batch['query_images'].squeeze(0)\n",
    "    query_masks = batch['query_masks'].squeeze(0)\n",
    "    return support_images, support_masks, query_images, query_masks\n",
    "\n",
    "# the function of copying the images\n",
    "def copy_file_to_folder(source_file, dest_folder):\n",
    "    if not os.path.exists(dest_folder):\n",
    "        os.makedirs(dest_folder)\n",
    "\n",
    "    dest_path = os.path.join(dest_folder, os.path.basename(source_file))\n",
    "    shutil.copy(source_file, dest_path)\n",
    "\n",
    "def evaluation_basenet(base_net,query_images,query_masks,criterion):\n",
    "        predicted = base_net(query_images)\n",
    "        loss = criterion(predicted,query_masks)\n",
    "        predicted = torch.argmax(predicted,dim=1).long()\n",
    "        predict_numpy = predicted.detach().cpu().numpy().reshape(-1)\n",
    "        masks_numpy = query_masks.long().detach().cpu().numpy().reshape(-1)\n",
    "        accuracy = metrics.accuracy_score(masks_numpy,predict_numpy)\n",
    "        f1_score = metrics.f1_score(masks_numpy,predict_numpy,average=None)\n",
    "        return accuracy,f1_score,loss\n",
    "\n",
    "def initialize_weights_he(model):\n",
    "    for param in model.parameters():\n",
    "        init.kaiming_uniform_(param, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "def initialize_weights_xavier(model):\n",
    "    for param in model.parameters():\n",
    "        init.xavier_uniform_(param)\n",
    "\n",
    "def initialize_weights_normal(model):\n",
    "    for param in model.parameters():\n",
    "        init.normal_(param, mean=0, std=1)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------Generating data----------#\n"
     ]
    }
   ],
   "source": [
    "print('#----------Generating data----------#')\n",
    "images_resources_path = \"./data/HAM10000/origin/images/\"         # the resource folder of images\n",
    "masks_resources_path = \"./data/HAM10000/origin/masks/\"           # the resource folder of masks\n",
    "ratio = 0.8     # the dataset and testset ratio\n",
    "categories = config.categories\n",
    "categories_dictionary = {}\n",
    "category_id = 1\n",
    "# prepare the csv for groundtruth\n",
    "origin_groundtruth_csv = \"./data/HAM10000/origin/groundtruth/HAM10000_groundtruth.csv\"   # read the csv file\n",
    "origin_groundtruth = pd.read_csv(origin_groundtruth_csv)    # read the csv file of groundtruth\n",
    "\n",
    "# generating the folders for each category in train folder and test folder\n",
    "# create folders for each categories\n",
    "trainset_images_path = \"./data/HAM10000/train/images/\"     # the images path for train dataset\n",
    "trainset_masks_path = \"./data/HAM10000/train/masks/\"     # the masks path for train dataset\n",
    "testset_images_path = \"./data/HAM10000/test/images/\"     # the images path for test dataset\n",
    "testset_masks_path = \"./data/HAM10000/test/masks/\"      # the masks path for test dataset\n",
    "\n",
    "for category in categories:\n",
    "    # prepare the address for folders\n",
    "    category_images_train_path = os.path.join(trainset_images_path,category)\n",
    "    category_masks_train_path = os.path.join(trainset_masks_path,category)\n",
    "    category_images_test_path = os.path.join(testset_images_path,category)\n",
    "    category_masks_test_path = os.path.join(testset_masks_path,category)\n",
    "    #delete the previously exsited folders\n",
    "    shutil.rmtree(category_images_train_path)\n",
    "    shutil.rmtree(category_masks_train_path)\n",
    "    shutil.rmtree(category_images_test_path)\n",
    "    shutil.rmtree(category_masks_test_path)\n",
    "    # create corresponding folder for each categories\n",
    "    os.makedirs(category_images_train_path, exist_ok=True)\n",
    "    os.makedirs(category_masks_train_path, exist_ok=True)\n",
    "    os.makedirs(category_images_test_path, exist_ok=True)\n",
    "    os.makedirs(category_masks_test_path, exist_ok=True)\n",
    "\n",
    "    # generate the data in trainset and testset for each categories\n",
    "    dest_folder_images = \"./data/HAM10000/train/images/\"+category    # the destination train set folder of copying the images\n",
    "    dest_folder_masks = \"./data/HAM10000/train/masks/\"+category    # the destination trian set folder of copying the masks\n",
    "    dest_folder_images_change = \"./data/HAM10000/test/images/\"+category     # the destination folder of test set images\n",
    "    dest_folder_masks_change = \"./data/HAM10000/test/masks/\"+category      # the destination folder of test set masks\n",
    "    data_categories = origin_groundtruth[origin_groundtruth['dx'] == category]      # extract each categories \n",
    "    length_categories = len(data_categories)\n",
    "    chaneg_folder_point = math.floor(length_categories * ratio)     # get the point to change directory name \n",
    "    elements_count = 0\n",
    "    for image_name in data_categories['image_id']:      # each image_id in each categories\n",
    "        if elements_count == chaneg_folder_point:\n",
    "            dest_folder_images = dest_folder_images_change\n",
    "            dest_folder_masks = dest_folder_masks_change\n",
    "        images_file = image_name+\".jpg\"\n",
    "        masks_file = image_name+\"_segmentation.png\"\n",
    "        source_image = images_resources_path+images_file    # the full path of source of image : path + image file name\n",
    "        source_mask = masks_resources_path+masks_file       # the full path of source of mask : path + mask file name\n",
    "        copy_file_to_folder(source_image,dest_folder_images)\n",
    "        # masks should be preprocess to the form of output for network (Width*Height*Category)\n",
    "        image = Image.open(source_mask)\n",
    "        image_array = np.array(image)\n",
    "        image_array[image_array == 255] = category_id\n",
    "        image = Image.fromarray(image_array)\n",
    "        image.save(os.path.join(dest_folder_masks, masks_file))\n",
    "        elements_count +=1\n",
    "    categories_dictionary[category] = category_id       # add the category id in the categories_dictionary\n",
    "    category_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------GPU init----------#\n"
     ]
    }
   ],
   "source": [
    "print('#----------GPU init----------#')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu_id\n",
    "set_seed(config.seed)\n",
    "device = torch.device('cuda')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------Prepareing Model----------#\n"
     ]
    }
   ],
   "source": [
    "print('#----------Prepareing Model----------#')\n",
    "in_channels = config.in_channels\n",
    "out_channels = config.num_classes\n",
    "# base_net = smp.Unet(encoder_name='resnet34', encoder_depth=5, encoder_weights=None, decoder_use_batchnorm=True, decoder_channels=(256, 128, 64, 32, 16), decoder_attention_type=None, in_channels=3, classes=config.num_classes, activation=None, aux_params=None)\n",
    "base_net = smp.UnetPlusPlus(encoder_name='resnet34', encoder_depth=5, encoder_weights=None, decoder_use_batchnorm=True, decoder_channels=(256, 128, 64, 32, 16), decoder_attention_type=None, in_channels=3, classes=config.num_classes, activation=None, aux_params=None)\n",
    "# initialize_weights_he(base_net)\n",
    "base_net = base_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------Prepareing loss, opt, sch and amp----------#\n"
     ]
    }
   ],
   "source": [
    "print('#----------Prepareing loss, opt, sch and amp----------#')\n",
    "criterion = config.criterion\n",
    "meta_optimizer = get_optimizer(config, base_net)\n",
    "meta_scheduler = get_scheduler(config, meta_optimizer)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------Set other params----------#\n"
     ]
    }
   ],
   "source": [
    "print('#----------Set other params----------#')\n",
    "min_loss = 999\n",
    "start_epoch = 1\n",
    "min_epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------Start training----------#\n",
      "batch_size:1, 3-way, 30-shot, 30-query, 128-resizeh, 128-resizew, 0.001000-outer_lr,0.000100-inner_lr\n",
      "Evaluation: Epoch: 1, loss: 1.0901482105255127, accuracy: 0.5734, f1_score: [0.74319156 0.09351521 0.03534878 0.19951566]\n",
      "Evaluation: Epoch: 2, loss: 0.9985557794570923, accuracy: 0.6148, f1_score: [0.78119745 0.13880876 0.05213635 0.23251754]\n",
      "Evaluation: Epoch: 3, loss: 0.9865462183952332, accuracy: 0.6212, f1_score: [0.78573036 0.23997494 0.04106322 0.26587072]\n",
      "Evaluation: Epoch: 4, loss: 0.9656497240066528, accuracy: 0.6414, f1_score: [0.80605017 0.31467288 0.05352986 0.22954968]\n",
      "Evaluation: Epoch: 5, loss: 0.960039496421814, accuracy: 0.6494, f1_score: [0.8038477  0.41693517 0.0483586  0.26537087]\n",
      "Evaluation: Epoch: 6, loss: 0.8315751552581787, accuracy: 0.7154, f1_score: [0.85846635 0.48386404 0.07398575 0.27530382]\n",
      "Evaluation: Epoch: 7, loss: 0.8509877324104309, accuracy: 0.7118, f1_score: [0.85178227 0.51924971 0.06705042 0.29772477]\n",
      "Evaluation: Epoch: 8, loss: 0.8848085403442383, accuracy: 0.6905, f1_score: [0.84858958 0.42004722 0.08354429 0.24838795]\n",
      "Evaluation: Epoch: 9, loss: 0.9147474765777588, accuracy: 0.6819, f1_score: [0.83789253 0.47777729 0.07004151 0.20982223]\n",
      "Evaluation: Epoch: 10, loss: 0.8384578227996826, accuracy: 0.7168, f1_score: [0.86493433 0.52252099 0.09318791 0.2494952 ]\n",
      "Evaluation: Epoch: 11, loss: 0.8361204266548157, accuracy: 0.7141, f1_score: [0.86336819 0.48121719 0.0899367  0.23845815]\n",
      "Evaluation: Epoch: 12, loss: 0.7808609008789062, accuracy: 0.7485, f1_score: [0.88505567 0.52963103 0.11202608 0.2356053 ]\n",
      "Evaluation: Epoch: 13, loss: 0.8211280107498169, accuracy: 0.7161, f1_score: [0.86702716 0.4542106  0.08474311 0.31174653]\n",
      "Evaluation: Epoch: 14, loss: 0.8501376509666443, accuracy: 0.7075, f1_score: [0.8550989  0.48845524 0.11131187 0.28132838]\n",
      "Evaluation: Epoch: 15, loss: 0.7685695886611938, accuracy: 0.7480, f1_score: [0.88338334 0.5781223  0.15836598 0.2835963 ]\n",
      "Evaluation: Epoch: 16, loss: 0.76164710521698, accuracy: 0.7535, f1_score: [0.8866139  0.53050061 0.11146349 0.34295725]\n",
      "Evaluation: Epoch: 17, loss: 0.7493726015090942, accuracy: 0.7667, f1_score: [0.89727566 0.57855876 0.13703821 0.27347194]\n",
      "Evaluation: Epoch: 18, loss: 0.7598387002944946, accuracy: 0.7482, f1_score: [0.88346489 0.49644727 0.14691191 0.32668403]\n",
      "Evaluation: Epoch: 19, loss: 0.7328749895095825, accuracy: 0.7635, f1_score: [0.88828637 0.56423309 0.173712   0.38533524]\n",
      "Evaluation: Epoch: 20, loss: 0.8081845641136169, accuracy: 0.7245, f1_score: [0.85855879 0.56367772 0.13347009 0.35211722]\n",
      "Evaluation: Epoch: 21, loss: 0.7786455154418945, accuracy: 0.7433, f1_score: [0.87344077 0.60824378 0.14162733 0.29713615]\n",
      "Evaluation: Epoch: 22, loss: 0.6999537944793701, accuracy: 0.7789, f1_score: [0.89949459 0.60138388 0.18295329 0.40462312]\n",
      "Evaluation: Epoch: 23, loss: 0.7411133646965027, accuracy: 0.7697, f1_score: [0.88792615 0.57414411 0.1542659  0.42320147]\n",
      "Evaluation: Epoch: 24, loss: 0.728676438331604, accuracy: 0.7660, f1_score: [0.88836732 0.59132759 0.20706686 0.36017851]\n",
      "Evaluation: Epoch: 25, loss: 0.7247649431228638, accuracy: 0.7666, f1_score: [0.89436341 0.53394022 0.22167117 0.30539677]\n",
      "Evaluation: Epoch: 26, loss: 0.8189273476600647, accuracy: 0.7137, f1_score: [0.85493482 0.49859892 0.2050792  0.34876258]\n",
      "Evaluation: Epoch: 27, loss: 0.7306450605392456, accuracy: 0.7611, f1_score: [0.88860071 0.6129992  0.17031184 0.36717162]\n",
      "Evaluation: Epoch: 28, loss: 0.6975186467170715, accuracy: 0.7785, f1_score: [0.89430628 0.58037878 0.29907914 0.4385939 ]\n",
      "Evaluation: Epoch: 29, loss: 0.7612944841384888, accuracy: 0.7458, f1_score: [0.87560282 0.56486634 0.21890459 0.32822309]\n",
      "Evaluation: Epoch: 30, loss: 0.6933236122131348, accuracy: 0.7780, f1_score: [0.9062133  0.54665949 0.22081168 0.42289134]\n",
      "Evaluation: Epoch: 31, loss: 0.7308182716369629, accuracy: 0.7662, f1_score: [0.88571609 0.6239805  0.2614364  0.37057338]\n",
      "Evaluation: Epoch: 32, loss: 0.7169336676597595, accuracy: 0.7691, f1_score: [0.88899127 0.59067918 0.31928004 0.39052878]\n",
      "Evaluation: Epoch: 33, loss: 0.6488747596740723, accuracy: 0.8057, f1_score: [0.91451848 0.64478457 0.3008044  0.51315815]\n",
      "Evaluation: Epoch: 34, loss: 0.6149193644523621, accuracy: 0.8122, f1_score: [0.91783527 0.62516573 0.39167628 0.53397394]\n",
      "Evaluation: Epoch: 35, loss: 0.7299949526786804, accuracy: 0.7609, f1_score: [0.88112354 0.57731309 0.33019519 0.46505799]\n",
      "Evaluation: Epoch: 36, loss: 0.6547742486000061, accuracy: 0.7915, f1_score: [0.90459014 0.65819976 0.36858976 0.37844333]\n",
      "Evaluation: Epoch: 37, loss: 0.6416146755218506, accuracy: 0.8000, f1_score: [0.91932446 0.55077579 0.34251713 0.41778921]\n",
      "Evaluation: Epoch: 38, loss: 0.6755810379981995, accuracy: 0.7891, f1_score: [0.89380283 0.62097062 0.36470963 0.51616916]\n",
      "Evaluation: Epoch: 39, loss: 0.6214975714683533, accuracy: 0.8077, f1_score: [0.91933865 0.62544155 0.38106388 0.48213178]\n",
      "Evaluation: Epoch: 40, loss: 0.5967695116996765, accuracy: 0.8211, f1_score: [0.91809013 0.6550012  0.46174135 0.55008498]\n",
      "Evaluation: Epoch: 41, loss: 0.5944892764091492, accuracy: 0.8195, f1_score: [0.91337047 0.66304987 0.46656601 0.52619158]\n",
      "Evaluation: Epoch: 42, loss: 0.5663939118385315, accuracy: 0.8343, f1_score: [0.93435329 0.63153089 0.48601957 0.51794145]\n",
      "Evaluation: Epoch: 43, loss: 0.6279811859130859, accuracy: 0.8104, f1_score: [0.90360757 0.64050321 0.49582604 0.58682635]\n",
      "Evaluation: Epoch: 44, loss: 0.6598963737487793, accuracy: 0.7934, f1_score: [0.89681551 0.61850857 0.51922178 0.4714682 ]\n",
      "Evaluation: Epoch: 45, loss: 0.6528058648109436, accuracy: 0.7940, f1_score: [0.89667119 0.64622506 0.51844557 0.47657639]\n",
      "Evaluation: Epoch: 46, loss: 0.5649188160896301, accuracy: 0.8394, f1_score: [0.91882121 0.73154091 0.62483086 0.51692341]\n",
      "Evaluation: Epoch: 47, loss: 0.6062745451927185, accuracy: 0.8168, f1_score: [0.90828185 0.67232547 0.58718204 0.55574605]\n",
      "Evaluation: Epoch: 48, loss: 0.5432223081588745, accuracy: 0.8505, f1_score: [0.94043585 0.69266473 0.54464372 0.54152537]\n",
      "Evaluation: Epoch: 49, loss: 0.5924272537231445, accuracy: 0.8205, f1_score: [0.91666268 0.60260663 0.52805431 0.5906317 ]\n",
      "Evaluation: Epoch: 50, loss: 0.4922410845756531, accuracy: 0.8761, f1_score: [0.94398429 0.80114594 0.57444494 0.66145006]\n",
      "Evaluation: Epoch: 51, loss: 0.5640295743942261, accuracy: 0.8465, f1_score: [0.91838149 0.73069851 0.66803133 0.62439448]\n",
      "Evaluation: Epoch: 52, loss: 0.5649506449699402, accuracy: 0.8364, f1_score: [0.92610916 0.65319356 0.58290593 0.62646534]\n",
      "Evaluation: Epoch: 53, loss: 0.539591908454895, accuracy: 0.8581, f1_score: [0.92693486 0.67355485 0.68341513 0.69961773]\n",
      "Evaluation: Epoch: 54, loss: 0.5040358901023865, accuracy: 0.8656, f1_score: [0.93314157 0.75901202 0.66208152 0.60832884]\n",
      "Evaluation: Epoch: 55, loss: 0.529329776763916, accuracy: 0.8573, f1_score: [0.92441585 0.75437962 0.66905534 0.64766835]\n",
      "Evaluation: Epoch: 56, loss: 0.49744006991386414, accuracy: 0.8695, f1_score: [0.93822392 0.72866017 0.6943243  0.64044133]\n",
      "Evaluation: Epoch: 57, loss: 0.5063669085502625, accuracy: 0.8687, f1_score: [0.93208768 0.79632851 0.70084911 0.61713961]\n",
      "Evaluation: Epoch: 58, loss: 0.5252487063407898, accuracy: 0.8584, f1_score: [0.92519912 0.75270239 0.6646306  0.67984938]\n",
      "Evaluation: Epoch: 59, loss: 0.489601731300354, accuracy: 0.8778, f1_score: [0.93595347 0.77834336 0.68019549 0.72414971]\n",
      "Evaluation: Epoch: 60, loss: 0.5671557188034058, accuracy: 0.8406, f1_score: [0.89849388 0.76168671 0.70595073 0.69100103]\n",
      "Evaluation: Epoch: 61, loss: 0.5004727244377136, accuracy: 0.8766, f1_score: [0.93142048 0.76977338 0.68912052 0.76227336]\n",
      "Evaluation: Epoch: 62, loss: 0.4743967652320862, accuracy: 0.8777, f1_score: [0.93158574 0.79033995 0.6416219  0.7757725 ]\n",
      "Evaluation: Epoch: 63, loss: 0.48250606656074524, accuracy: 0.8744, f1_score: [0.93339144 0.76563009 0.70986768 0.71015288]\n",
      "Evaluation: Epoch: 64, loss: 0.523125171661377, accuracy: 0.8619, f1_score: [0.92497208 0.7221442  0.71591623 0.7604969 ]\n",
      "Evaluation: Epoch: 65, loss: 0.5407251715660095, accuracy: 0.8484, f1_score: [0.92015391 0.70788087 0.71431284 0.67923596]\n",
      "Evaluation: Epoch: 66, loss: 0.48613879084587097, accuracy: 0.8758, f1_score: [0.93078544 0.78097453 0.72366011 0.76469694]\n",
      "Evaluation: Epoch: 67, loss: 0.49787643551826477, accuracy: 0.8587, f1_score: [0.92198661 0.79950363 0.73630419 0.60663833]\n",
      "Evaluation: Epoch: 68, loss: 0.5078164339065552, accuracy: 0.8696, f1_score: [0.91865778 0.77468224 0.7953123  0.72008906]\n",
      "Evaluation: Epoch: 69, loss: 0.44388166069984436, accuracy: 0.9017, f1_score: [0.94074334 0.84300118 0.78735827 0.8142797 ]\n",
      "Evaluation: Epoch: 70, loss: 0.48018231987953186, accuracy: 0.8799, f1_score: [0.92359931 0.77523985 0.80026983 0.76175424]\n",
      "Evaluation: Epoch: 71, loss: 0.4663555920124054, accuracy: 0.8722, f1_score: [0.92644589 0.73903919 0.69800518 0.75953197]\n",
      "Evaluation: Epoch: 72, loss: 0.44180989265441895, accuracy: 0.8877, f1_score: [0.93253824 0.81005334 0.780628   0.76792887]\n",
      "Evaluation: Epoch: 73, loss: 0.39197036623954773, accuracy: 0.9113, f1_score: [0.9497774  0.84869345 0.79712388 0.80076039]\n",
      "Evaluation: Epoch: 74, loss: 0.39564985036849976, accuracy: 0.9128, f1_score: [0.95103998 0.83192186 0.77245554 0.83714574]\n",
      "Evaluation: Epoch: 75, loss: 0.3927246928215027, accuracy: 0.9143, f1_score: [0.94997278 0.81675083 0.8249269  0.8283223 ]\n",
      "Evaluation: Epoch: 76, loss: 0.4014292359352112, accuracy: 0.9089, f1_score: [0.94429073 0.83379609 0.80145205 0.84390212]\n",
      "Evaluation: Epoch: 77, loss: 0.4514184594154358, accuracy: 0.8907, f1_score: [0.93226467 0.76476935 0.79234988 0.83618249]\n",
      "Evaluation: Epoch: 78, loss: 0.4901394248008728, accuracy: 0.8666, f1_score: [0.90811602 0.80063467 0.79565878 0.75452238]\n",
      "Evaluation: Epoch: 79, loss: 0.4461621940135956, accuracy: 0.8829, f1_score: [0.93499225 0.78254644 0.78815953 0.72619619]\n",
      "Evaluation: Epoch: 80, loss: 0.43184995651245117, accuracy: 0.8871, f1_score: [0.92678879 0.82340261 0.79998995 0.78316826]\n",
      "Evaluation: Epoch: 81, loss: 0.386339008808136, accuracy: 0.9078, f1_score: [0.94749127 0.8290278  0.77612623 0.81494928]\n",
      "Evaluation: Epoch: 82, loss: 0.3981172740459442, accuracy: 0.9041, f1_score: [0.93790554 0.84938366 0.82754972 0.81410793]\n",
      "Evaluation: Epoch: 83, loss: 0.4170885682106018, accuracy: 0.8833, f1_score: [0.93172187 0.78535589 0.75661654 0.77357488]\n",
      "Evaluation: Epoch: 84, loss: 0.3830055296421051, accuracy: 0.9120, f1_score: [0.94405012 0.86041916 0.83115461 0.8167247 ]\n",
      "Evaluation: Epoch: 85, loss: 0.3765226900577545, accuracy: 0.9162, f1_score: [0.94562645 0.85091436 0.847028   0.85326028]\n",
      "Evaluation: Epoch: 86, loss: 0.39964866638183594, accuracy: 0.9022, f1_score: [0.9364762  0.84840625 0.80787646 0.84808454]\n",
      "Evaluation: Epoch: 87, loss: 0.4125058948993683, accuracy: 0.8846, f1_score: [0.93338871 0.78196502 0.77822605 0.76787106]\n",
      "Evaluation: Epoch: 88, loss: 0.3620627820491791, accuracy: 0.9161, f1_score: [0.94688425 0.86712318 0.82429411 0.84632824]\n",
      "Evaluation: Epoch: 89, loss: 0.36991581320762634, accuracy: 0.9141, f1_score: [0.94688676 0.84963922 0.81391362 0.8329784 ]\n",
      "Evaluation: Epoch: 90, loss: 0.39685338735580444, accuracy: 0.8976, f1_score: [0.92848724 0.87313493 0.80439869 0.83360039]\n",
      "Evaluation: Epoch: 91, loss: 0.38339173793792725, accuracy: 0.9005, f1_score: [0.93194421 0.86126876 0.8110035  0.83272689]\n",
      "Evaluation: Epoch: 92, loss: 0.3745858073234558, accuracy: 0.9091, f1_score: [0.94216111 0.82711243 0.83420932 0.84858812]\n",
      "Evaluation: Epoch: 93, loss: 0.3670729398727417, accuracy: 0.9125, f1_score: [0.94411235 0.8605434  0.82226446 0.84698417]\n",
      "Evaluation: Epoch: 94, loss: 0.344747930765152, accuracy: 0.9163, f1_score: [0.94620005 0.86227216 0.7919492  0.88676167]\n",
      "Evaluation: Epoch: 95, loss: 0.33168208599090576, accuracy: 0.9268, f1_score: [0.95730614 0.86827598 0.84023996 0.8508043 ]\n",
      "Evaluation: Epoch: 96, loss: 0.37488865852355957, accuracy: 0.9087, f1_score: [0.94185958 0.85926816 0.83219329 0.8410448 ]\n",
      "Evaluation: Epoch: 97, loss: 0.44412484765052795, accuracy: 0.8682, f1_score: [0.91179721 0.80616164 0.73871403 0.79683293]\n",
      "Evaluation: Epoch: 98, loss: 0.34853866696357727, accuracy: 0.9100, f1_score: [0.94573132 0.86845858 0.85173617 0.77088629]\n",
      "Evaluation: Epoch: 99, loss: 0.3205749988555908, accuracy: 0.9303, f1_score: [0.95635422 0.8904732  0.86991109 0.85673577]\n",
      "Evaluation: Epoch: 100, loss: 0.34790724515914917, accuracy: 0.9079, f1_score: [0.95045561 0.8328441  0.78065449 0.81242961]\n"
     ]
    }
   ],
   "source": [
    "print('#----------Start training----------#')\n",
    "print(\n",
    "            \"batch_size:%d, %d-way, %d-shot, %d-query, %d-resizeh, %d-resizew, %f-outer_lr,%f-inner_lr\"\n",
    "            % (\n",
    "                config.batch_size,\n",
    "                config.n_way,\n",
    "                config.k_shot,\n",
    "                config.k_query,\n",
    "                config.resize_h,\n",
    "                config.resize_w,\n",
    "                config.inner_lr,\n",
    "                config.outer_lr\n",
    "            )\n",
    "        )\n",
    "torch.cuda.empty_cache()\n",
    "for epoch in range(start_epoch, config.epoch_num+1):\n",
    "    step = 0        # according to the step, decide to print the result or do the evaluation\n",
    "    # create the dataset and dataloader\n",
    "    train_dataset = HAM_datasets(config, train=True)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=config.dataloader_bs, num_workers=config.num_workers)\n",
    "    test_dataset = HAM_datasets(config, train=False)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=config.dataloader_bs, num_workers=config.num_workers)\n",
    "    # numerate to train\n",
    "    for i,batch in enumerate(train_dataset):\n",
    "        # claer the meta_optimizer, setting zero\n",
    "        meta_optimizer.zero_grad()\n",
    "        support_images, support_masks, query_images, query_masks = preprocess_batch(batch)\n",
    "        support_images, support_masks, query_images, query_masks = support_images.to(device), support_masks.to(device), query_images.to(device), query_masks.to(device)\n",
    "        support_masks = torch.squeeze(support_masks,dim=1).long()\n",
    "        query_masks = torch.squeeze(query_masks,dim=1).long()\n",
    "        # deep copy the base_net for inner loop, preparing the loss and optimizer\n",
    "        temp_net = deepcopy(base_net)\n",
    "        inner_optimizer = optim.SGD(temp_net.parameters(), lr=config.inner_lr)\n",
    "        temp_loss = None\n",
    "        for inner_step in range(config.inner_steps):\n",
    "            inner_optimizer.zero_grad()\n",
    "            predicted = temp_net(support_images)\n",
    "            loss = criterion(predicted,support_masks)\n",
    "            loss.backward()\n",
    "            inner_optimizer.step()\n",
    "        query_predicted = temp_net(query_images)\n",
    "        meta_loss = criterion(query_predicted,query_masks)\n",
    "        meta_loss.backward()\n",
    "        with torch.no_grad():\n",
    "                for param1, param2 in zip(temp_net.parameters(), base_net.parameters()):\n",
    "                    if param1.grad is not None:\n",
    "                        param2.grad = param1.grad.clone()\n",
    "        meta_optimizer.step()\n",
    "    with torch.no_grad():\n",
    "        accuracy,f1_score,loss_score = evaluation_basenet(base_net,query_images,query_masks,criterion)\n",
    "        log_info = f'Epoch: {epoch}, loss: {loss_score}, accuracy: {accuracy:.4f}, f1_score: {f1_score}'\n",
    "        print(\"Evaluation:\",log_info)\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataEngineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
